{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "058410cb",
   "metadata": {},
   "source": [
    "Instructions to run:\n",
    "    Ensure the data set is stored in the working directory for the python environment with the name raw_notes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d7a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all necessary modules at the start\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import string\n",
    "import scipy\n",
    "import contextualSpellCheck\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from wordcloud import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA \n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab766ff9",
   "metadata": {},
   "source": [
    "Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac7056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load stop words, punctuations and start Spacy. Load data from csv\n",
    "punctuations = string.punctuation\n",
    "en_spa = spacy.load('en_core_web_sm')\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "data = pd.read_csv('raw_notes.csv')\n",
    "contextualSpellCheck.add_to_pipe(en_spa)\n",
    "data = data[['Course','Sentence','Topic']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b494298",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('raw_notes.csv').dropna()\n",
    "tex = ''.join(str(data['Sentence'])).replace(\"\\n\", \" \").replace(\"\\r\", \" \") #Remove line breaks and ensure \n",
    "tex = re.sub(\"[^a-zA-Z#]\",\" \",tex) # Only alphabetical characters remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tex # Print out the start of the text string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d11cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(tex) #Generate a wordcloud for the full corpus\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('Wordcloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85f729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "UDAtex = data[(data.Course==\"Unstructured Data Analysis\")]['Sentence'] #Create a text for each course\n",
    "ULtex  = data[(data.Course==\"Unsupervised Learning\\n\")]['Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3513343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "UDAtex = ''.join(str(UDAtex)).replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "UDAtex = re.sub(\"[^a-zA-Z#]\",\" \",UDAtex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc8e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "ULtex = ''.join(str(ULtex)).replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "ULtex = re.sub(\"[^a-zA-Z#]\",\" \",ULtex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19735168",
   "metadata": {},
   "outputs": [],
   "source": [
    "UDAtex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3595bdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ULtex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a57f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_UDA = WordCloud().generate(UDAtex) #Create word cloud for each course text\n",
    "wordcloud_UL = WordCloud().generate(ULtex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903384c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud_UDA, interpolation='bilinear') #Save each wordcloud\n",
    "plt.axis(\"off\") \n",
    "plt.savefig('Wordcloud_UDA.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07681567",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud_UL, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('Wordcloud_UL.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ede5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    # Create token object from spacy with no stopwords and lemmatize words that are not proper nouns\n",
    "    docs = en_spa(sentence)\n",
    "    tokens = docs._.outcome_spellCheck # Perform spell check\n",
    "    tokens = en_spa(tokens)\n",
    "    tokens = [word.lemma_.lower().strip() if word.lemma_ != \"PROPN\" else word.lower_ for word in docs]\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "    tokens = [word for word in tokens if word not in punctuations] # Remove punctuations\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79992ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create vectorizers for the different N-Grams required\n",
    "vectorizer = TfidfVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
    "count_vec = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
    "vectorizer2 = TfidfVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,2))\n",
    "vectorizer3 = TfidfVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,3))\n",
    "count_vec2 = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,2))\n",
    "count_vec3 = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40906db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sparse matrices from vectorizers intialised in the cell above\n",
    "X = data['Sentence']\n",
    "X = vectorizer.fit_transform(X)\n",
    "Y2 = data['Sentence']\n",
    "Y2 = vectorizer2.fit_transform(Y2)\n",
    "Y5 = data['Sentence']\n",
    "Y5 = vectorizer3.fit_transform(Y5)\n",
    "X2 = data['Sentence']\n",
    "X2 = count_vec.fit_transform(X2)\n",
    "Y3 = data['Sentence']\n",
    "Y3 = count_vec2.fit_transform(Y3)\n",
    "Y4= data['Sentence']\n",
    "Y4 = count_vec3.fit_transform(Y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e740d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to long computation time, save the vectors as a pickle \n",
    "pickle.dump(vectorizer, open(\"vectorizer.pickle\", \"wb\"))\n",
    "pickle.dump(vectorizer2, open(\"vectorizer2.pickle\", \"wb\"))\n",
    "pickle.dump(vectorizer3, open(\"vectorizer3.pickle\", \"wb\"))\n",
    "pickle.dump(count_vec, open(\"vectorizer4.pickle\", \"wb\"))\n",
    "pickle.dump(count_vec2, open(\"vectorizer5.pickle\", \"wb\"))\n",
    "pickle.dump(count_vec3, open(\"vectorizer6.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f1360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform initial SVD to understand the number of components required in LSA\n",
    "svd = TruncatedSVD(n_components=1000, n_iter=20, random_state=2023)\n",
    "svd.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the explained variance by dimensions\n",
    "plt.plot(svd.explained_variance_ratio_)\n",
    "plt.title(\"explained variance ratio\")\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.show()\n",
    "plt.savefig('ExplainedVariance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011995b9",
   "metadata": {},
   "source": [
    "Unigram LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb13e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform LSA for unigram vectorized dataset and output the topics and words within topics\n",
    "svd = TruncatedSVD(n_components=100, n_iter=20, random_state=2023)\n",
    "X_ = svd.fit_transform(X)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i, comp in enumerate(svd.components_):\n",
    "    if(i>10):\n",
    "        break\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:5]\n",
    "    print(\"Topic Number \"+str(i)+\": \")\n",
    "    for t in sorted_terms:\n",
    "        print(t[0])\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e2f0c9",
   "metadata": {},
   "source": [
    "Unigram + Bigram LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform LSA for unigram+bigram vectorized dataset and output the topics and words within topics\n",
    "\n",
    "svd = TruncatedSVD(n_components=1000, n_iter=20, random_state=2023)\n",
    "svd.fit(Y2)\n",
    "\n",
    "plt.plot(svd.explained_variance_ratio_)\n",
    "plt.title(\"explained variance ratio TFID 2gram\")\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.show()\n",
    "plt.xlim(-10,100)\n",
    "plt.savefig('ExplainedVariance2gram.png')\n",
    "\n",
    "svd = TruncatedSVD(n_components=10, n_iter=20, random_state=2023)\n",
    "X_ = svd.fit_transform(Y2)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i, comp in enumerate(svd.components_):\n",
    "    if(i>10):\n",
    "        break\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:6]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_terms:\n",
    "        print(t[0])\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec36319",
   "metadata": {},
   "source": [
    "Unigram + Bigram + Trigram LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform LSA for unigram+bigram+trigram vectorized dataset and output the topics and words within topics\n",
    "svd = TruncatedSVD(n_components=1000, n_iter=20, random_state=2023)\n",
    "svd.fit(Y3)\n",
    "\n",
    "plt.plot(svd.explained_variance_ratio_)\n",
    "plt.title(\"explained variance ratio\")\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.show()\n",
    "plt.savefig('ExplainedVariance.png')\n",
    "\n",
    "svd = TruncatedSVD(n_components=100, n_iter=20, random_state=2023)\n",
    "X_ = svd.fit_transform(X)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i, comp in enumerate(svd.components_):\n",
    "    if(i>10):\n",
    "        break\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:5]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_terms:\n",
    "        print(t[0])\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeaad4d",
   "metadata": {},
   "source": [
    "LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d41406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perfrom LDA for the dataset to compare topics to LSA\n",
    "lda_model = LatentDirichletAllocation(n_components=10, random_state=0)\n",
    "lda_model.fit(X)\n",
    "\n",
    "# print the topics learned by the model\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx))\n",
    "    print(\" \".join([count_vec.get_feature_names_out()[i]\n",
    "                    for i in topic.argsort()[:-10 - 1:-1]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb933d0",
   "metadata": {},
   "source": [
    "Logistic Regression (Unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b264ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform logistic regression on the unigram dataset and output the accuracy and F1 score\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,data['Course'],random_state=2023, test_size=0.2)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = (clf.predict(X_test))\n",
    "\n",
    "accuracy_score(y_test,y_pred)\n",
    "\n",
    "f1_score(y_test,y_pred,pos_label=\"Unstructured Data Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a26cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction scores for new texts\n",
    "new = ['Pixels are assigned integer values between 0 and 255','NMF provides an approximate decomposition of the data matrix']\n",
    "X_New = vectorizer.transform(new)\n",
    "clf.predict_proba(X_New)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67737ab",
   "metadata": {},
   "source": [
    "SVC (Unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cc8c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVM Classification on the unigram dataset and output the accuracy and F1 score\n",
    "clf2 = SVC(probability=True)\n",
    "\n",
    "clf2.fit(X_train,y_train)\n",
    "y_pred = (clf2.predict(X_test))\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "\n",
    "print(f1_score(y_test,y_pred,pos_label=\"Unstructured Data Analysis\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dab70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2.predict_proba(X_New)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the AUC score for the above binary classification models\n",
    "preds1 = clf.predict_proba(X_test)\n",
    "preds2 = clf2.predict_proba(X_test)\n",
    "fpr1, tpr1, thresh1 = roc_curve(y_test, preds1[:,0], pos_label=\"Unstructured Data Analysis\")\n",
    "fpr2, tpr2, thresh2 = roc_curve(y_test, preds2[:,0], pos_label=\"Unstructured Data Analysis\")\n",
    "probs = [0 for i in range(len(y_test))]\n",
    "p_fpr, p_tpr, _ = roc_curve(y_test, probs, pos_label=1)\n",
    "auc_score1 = roc_auc_score(y_test, preds1[:,0])\n",
    "auc_score2 = roc_auc_score(y_test, preds2[:,0])\n",
    "\n",
    "print(auc_score1, auc_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3969fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.plot(fpr1, tpr1, color='red', label='Logistic Regression')\n",
    "plt.plot(fpr2, tpr2, color='blue', label='SVM')\n",
    "plt.plot(p_fpr, p_tpr, color='green')\n",
    "plt.title('ROC curve Comparison')\n",
    "plt.legend()\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.savefig('ROC_AUC.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbffb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform \n",
    "X_train, X_test, y_train,y_test = train_test_split(X,data['Topic'],random_state=2023, test_size=0.2)\n",
    "clf3 = MultinomialNB()\n",
    "clf3.fit(X_train,y_train)\n",
    "y_pred = (clf3.predict(X_test))\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f6ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC \n",
    "X_train, X_test, y_train,y_test = train_test_split(X,data['Topic'],random_state=2023, test_size=0.2)\n",
    "clf4 = LinearSVC()\n",
    "clf4.fit(X_train,y_train)\n",
    "y_pred = (clf4.predict(X_test))\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9b3ff",
   "metadata": {},
   "source": [
    "K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd5422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans # initialize KMeans with 4 clusters\n",
    "kmeans = KMeans(verbose=1,n_clusters=4) \n",
    "kmeans.fit(X)\n",
    "clusters = kmeans.labels_\n",
    "# Output a few words for each cluster (example given in lecture notes for Text clustering)\n",
    "df = pd.DataFrame(X.todense()).groupby(clusters).mean() \n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i,r in df.iterrows():\n",
    "    print('\\nCluster Number {}'.format(i))\n",
    "    print(','.join([terms[t] for t in np.argsort(r)[-10:]]))\n",
    "\n",
    "pca = PCA(n_components=2, random_state=20)\n",
    "pca_vecs=pca.fit_transform(X.toarray())\n",
    "x0 = pca_vecs[:,0]\n",
    "x1 = pca_vecs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8ef9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot for K means clustering\n",
    "d = {'Cluster':clusters.tolist(), 'x0':x0, 'x1':x1,'Topic':data['Topic']}\n",
    "df3 = pd.DataFrame(data=d)\n",
    "groups = df3.groupby('Topic')\n",
    "fig, ax = plt.subplots()\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x0, group.x1, ms=6, label=name)\n",
    "\n",
    "for i, txt in enumerate(clusters):\n",
    "    ax.annotate(txt, (x0[i], x1[i]))\n",
    "ax.legend()\n",
    "plt.title('K-Means Cluster with single words')\n",
    "plt.savefig('K-meansWords.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a6876e",
   "metadata": {},
   "source": [
    "Repeat for Unigram + Bigram and Unigram + Bigram + Trigram datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9406c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans # initialize KMeans with 4 clusters\n",
    "kmeans = KMeans(verbose=1,n_clusters=4) \n",
    "kmeans.fit(Y2)\n",
    "clusters = kmeans.labels_\n",
    "# Output a few words for each cluster (example given in lecture notes for Text clustering)\n",
    "df = pd.DataFrame(Y2.todense()).groupby(clusters).mean() \n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i,r in df.iterrows():\n",
    "    print('\\nCluster Number {}'.format(i))\n",
    "    print(','.join([terms[t] for t in np.argsort(r)[-10:]]))\n",
    "\n",
    "pca = PCA(n_components=2, random_state=20)\n",
    "pca_vecs=pca.fit_transform(Y2.toarray())\n",
    "x0 = pca_vecs[:,0]\n",
    "x1 = pca_vecs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffafd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot for K means clustering\n",
    "d = {'Cluster':clusters.tolist(), 'x0':x0, 'x1':x1,'Topic':data['Topic']}\n",
    "df3 = pd.DataFrame(data=d)\n",
    "groups = df3.groupby('Topic')\n",
    "fig, ax = plt.subplots()\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x0, group.x1, ms=6, label=name)\n",
    "\n",
    "for i, txt in enumerate(clusters):\n",
    "    ax.annotate(txt, (x0[i], x1[i]))\n",
    "ax.legend()\n",
    "plt.title('K-Means Cluster with single words')\n",
    "plt.savefig('K-meansWords.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d36aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans # initialize KMeans with 4 clusters\n",
    "kmeans = KMeans(verbose=1,n_clusters=4) \n",
    "kmeans.fit(Y5)\n",
    "clusters = kmeans.labels_\n",
    "# Output a few words for each cluster (example given in lecture notes for Text clustering)\n",
    "df = pd.DataFrame(Y5.todense()).groupby(clusters).mean() \n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i,r in df.iterrows():\n",
    "    print('\\nCluster Number {}'.format(i))\n",
    "    print(','.join([terms[t] for t in np.argsort(r)[-10:]]))\n",
    "\n",
    "pca = PCA(n_components=2, random_state=20)\n",
    "pca_vecs=pca.fit_transform(Y5.toarray())\n",
    "x0 = pca_vecs[:,0]\n",
    "x1 = pca_vecs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedc5953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot for K means clustering\n",
    "d = {'Cluster':clusters.tolist(), 'x0':x0, 'x1':x1,'Topic':data['Topic']}\n",
    "df3 = pd.DataFrame(data=d)\n",
    "groups = df3.groupby('Topic')\n",
    "fig, ax = plt.subplots()\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x0, group.x1, ms=6, label=name)\n",
    "\n",
    "for i, txt in enumerate(clusters):\n",
    "    ax.annotate(txt, (x0[i], x1[i]))\n",
    "ax.legend()\n",
    "plt.title('K-Means Cluster with single words')\n",
    "plt.savefig('K-meansWords.png')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
