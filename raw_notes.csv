Course,Topic,Term,Year,Sentence
Unstructured Data Analysis,Images,4,2,"A graph G is a finite nonempty set of objects, called vertices (the singular is vertex), together with a (possibly empty) set of unordered pairs of distinct vertices, called edges"
Unstructured Data Analysis,Images,4,2,"The objective world is three-dimensional (3D) in space, but the image obtained from the objective scene is generally two-dimensional (2D)"
Unstructured Data Analysis,Images,4,2,"An image can be represented by a 2D array f(x,y), where x and y represent the position of a coordinate point in the 2D space XY , and f represents the image value of a property F at a certain point (x, y)"
Unstructured Data Analysis,Images,4,2,"For example, f in a grayscale image represents a gray value, which often corresponds to the observed brightness of an objective scene"
Unstructured Data Analysis,Images,4,2,"Text images are often binary images, and there are only two values for f, corresponding to text and blank space, respectively"
Unstructured Data Analysis,Images,4,2,"The image at the point (x, y) can also have multiple properties at the same time"
Unstructured Data Analysis,Images,4,2,"In this case, it can be represented by a vector f"
Unstructured Data Analysis,Images,4,2,"For example, for a color image that has three values of red, green, and blue at each image point, this image can be recorded as [fr(x,y),fg(x,y),fb(x,y)]"
Unstructured Data Analysis,Images,4,2,"Images may have different properties at different positions/regions within the image, so several different techniques may be applied in a single image, depending on properties of certain regions"
Unstructured Data Analysis,Images,4,2,Caution! We need to be careful when representing images as arrays or matrices because array operations and matrix operations are different.
Unstructured Data Analysis,Images,4,2,"There are many different file formats to store images; the format that images are stored in, or the format of image to choose, largely depends on what we’d like to do with them image"
Unstructured Data Analysis,Images,4,2,"Raster images are made up of pixels, usually for storing and processing, while vector images are made up of mathematical curves and paths, for example, for resizing"
Unstructured Data Analysis,Images,4,2,These two image types are complementary
Unstructured Data Analysis,Images,4,2,A .png file consists of a png signature followed by a series of chunks. The first eight bytes of a png file always contain the following (decimal) values:
Unstructured Data Analysis,Images,4,2,"This signature indicates that the remainder of the file contains a single png image, consisting of a series of chunks beginning with an IHDR (image header) chunk and ending with an IEND (image end) chunk. You can see the online webpage for a complete description of the chunk layout."
Unstructured Data Analysis,Images,4,2,"Intuitively, spatial resolution is a measure of the smallest discernible detail in an image."
Unstructured Data Analysis,Images,4,2,"Quantitatively, spatial resolution can be captured in several ways—line pairs per unit distance, and dots (pixels) per unit distance are common measures."
Unstructured Data Analysis,Images,4,2,"Suppose that we construct a chart with alternating black and white vertical lines, each of width W units (W can be less than 1)."
Unstructured Data Analysis,Images,4,2,"The width of a line pair is thus 2W , and there are 1/2W line pairs per unit distance."
Unstructured Data Analysis,Images,4,2,"If the width of a line is 0.1 mm, there are 5 line pairs per unit distance (i.e., per mm)."
Unstructured Data Analysis,Images,4,2,"A widely used definition of image resolution is the largest number of discernible line pairs per unit distance (e.g. , 100 line pairs per mm)"
Unstructured Data Analysis,Images,4,2,Dots per unit distance is a measure of image resolution used in the printing and publishing industry
Unstructured Data Analysis,Images,4,2,"In the U.S., this measure usually is expressed as dots per inch (dpi)"
Unstructured Data Analysis,Images,4,2,"To give you an idea of quality, newspapers are printed with a resolution of 75 dpi, magazines at 133 dpi, glossy brochures at 175 dpi, and book pages at 2400 dpi"
Unstructured Data Analysis,Images,4,2,"A pixel p at coordinates (x, y) has two horizontal and two vertical neighbors with coordinates (x+1,y), (x−1,y), (x,y+1), (x,y−1)."
Unstructured Data Analysis,Images,4,2,"This set of pixels, called the 4-neighbors of p, is denoted N4(p)."
Unstructured Data Analysis,Images,4,2,"The four diagonal neighbors of p have coordinates (x+1,y+1), (x+1,y−1), (x−1,y+1), (x−1,y−1) and are denoted by ND(p)"
Unstructured Data Analysis,Images,4,2,"These neighbors, together with the 4-neighbors, are called the 8- neighbors of p, denoted by N8(p)."
Unstructured Data Analysis,Images,4,2,The set of image locations of the neighbors of a point p is called the neighborhood of p.
Unstructured Data Analysis,Images,4,2,The neighborhood is said to be closed if it contains p.
Unstructured Data Analysis,Images,4,2,"Otherwise, the neighborhood is said to be open"
Unstructured Data Analysis,Images,4,2,Let V be the set of intensity values used to define adjacency
Unstructured Data Analysis,Images,4,2,"In a binary image, V = {1} if we are referring to adjacency of pixels with value 1"
Unstructured Data Analysis,Images,4,2,"In a grayscale image, the idea is the same, but the set V typically contains more elements"
Unstructured Data Analysis,Images,4,2,"For example, if we are dealing with the adjacency of pixels whose values are in the range 0 to 255, the set V could be any subset of these 256 values"
Unstructured Data Analysis,Images,4,2,"Two pixels p and q with values from V are m-adjacent (or mixed adjacent) if
1. qisinN4(p),or
2. q is in ND(p) and the set N4(p) ∩ N4(q) has no pixels whose values are from V ."
Unstructured Data Analysis,Images,4,2,"Mixed adjacency is a modification of 8-adjacency, and is introduced to eliminate the ambigui- ties that may result from using 8-adjacency."
Unstructured Data Analysis,Images,4,2,"Connectivity: A digital path (or curve) from pixel p with coordinates (x0,y0) to pixel q with coordinates (xn, yn) is a sequence of distinct pixels with coordinates
(x0,y0), (x1,y1), ...,(xn,yn)
where points (xi, yi) and (xi−1, yi−1) are adjacent for 1 ≤ i ≤ n."
Unstructured Data Analysis,Images,4,2,"If (x0, y0) = (xn, yn), the path is a closed path."
Unstructured Data Analysis,Images,4,2,"We can define 4-, 8-, or m-paths, depending on the type of adjacency specified."
Unstructured Data Analysis,Images,4,2,Let S represent a subset of pixels in an image
Unstructured Data Analysis,Images,4,2,Two pixels p and q are said to be connected in S if there exists a path between them consisting entirely of pixels in S
Unstructured Data Analysis,Images,4,2,"For any pixel p in S, the set of pixels that are connected to it in S is called a connected component of S"
Unstructured Data Analysis,Images,4,2,"If it only has one component, and that component is connected, then S is called a connected set"
Unstructured Data Analysis,Images,4,2,Let R represent a subset of pixels in an image
Unstructured Data Analysis,Images,4,2,We call R a region of the image if R is a connected set
Unstructured Data Analysis,Images,4,2,Two regions Ri and Rj are said to be adjacent if their union forms a connected set
Unstructured Data Analysis,Images,4,2,Regions that are not adjacent are said to be disjoint
Unstructured Data Analysis,Images,4,2,We consider 4- and 8-adjacency when referring to regions
Unstructured Data Analysis,Images,4,2,"For our definition to make sense, the type of adjacency used must be specified"
Unstructured Data Analysis,Images,4,2,We use geometric transformations modify the spatial arrangement of pixels in an image
Unstructured Data Analysis,Images,4,2,"We might want to modify spatial arrangements of pixels on an image depending on usage or context, for example, when web-browsing on a computer versus mobile phone or we might like to view an image from another angle or perspective"
Unstructured Data Analysis,Images,4,2,"These transformations are often called rubber-sheet transformations because they may be viewed as analogous to “printing” an image on a rubber sheet, then stretching or shrinking the sheet according to a predefined set of rules"
Unstructured Data Analysis,Images,4,2,Geometric transformations of digital images consist of two basic operations:
Unstructured Data Analysis,Images,4,2,"In this module, we will restrict our attention to affine transformations, which include scaling, translation, rotation, and shearing"
Unstructured Data Analysis,Images,4,2,"The key characteristic of an affine transformation in 2D is that it preserves points, straight lines, and planes"
Unstructured Data Analysis,Images,4,2,"The above equation (1) can be used to express the transformations just mentioned, except translation, which would require that a constant 2D vector be added to the right-hand side of the equation"
Unstructured Data Analysis,Images,4,2,"The most popular technique for working with image/video data nowadays is deep learning, so we will talk about this first and demonstrate how deep learning can be used in imaging tasks"
Unstructured Data Analysis,Images,4,2,"Applications of CNN include image classification, image semantic segmentation, object detection in images, and many more"
Unstructured Data Analysis,Images,4,2,"As a running example through this section to illustrate CNNs and how they’ll work, we’ll use the task of image classification or categorization"
Unstructured Data Analysis,Images,4,2,"In image classification, every image has a major object which occupies a large portion of the image which is then classified into one of the classes based on the identity of its main object, e.g. , dog, airplane, bird, etc"
Unstructured Data Analysis,Images,4,2,"A CNN usually takes an order 3 tensor as its input, e.g. , an image with H rows, W columns, and 3 channels (R,G,B color channels)"
Unstructured Data Analysis,Images,4,2,Higher order tensor inputs can also be handled by CNNs similarly
Unstructured Data Analysis,Images,4,2,The input then sequentially goes through a series of processing
Unstructured Data Analysis,Images,4,2,"One processing step is called a layer, which could be a convolution layer, a pooling layer, a normalization layer, a fully connected layer, a loss layer, etc"
Unstructured Data Analysis,Images,4,2,Let’s suppose one training example x1 is given to train our parameters
Unstructured Data Analysis,Images,4,2,The training process involves running the CNN flow in both directions
Unstructured Data Analysis,Images,4,2,We first run the network in the forward pass to get xL to achieve a prediction using the current CNN parameters
Unstructured Data Analysis,Images,4,2,"Instead of outputting a prediction, we need to compare the prediction with the target t corresponding to x1—that is, continue running the forward pass until the last loss layer"
Unstructured Data Analysis,Images,4,2,"Finally, we achieve a loss z"
Unstructured Data Analysis,Images,4,2,"The loss z is then a supervision signal, guiding how the parameters of the model should be modified (updated)"
Unstructured Data Analysis,Images,4,2,"In order to minimize the loss function, we should update wi along the opposite direction of the gradient"
Unstructured Data Analysis,Images,4,2,This updating rule is called the gradient descent
Unstructured Data Analysis,Images,4,2,We will use PyTorch to build our CNN model
Unstructured Data Analysis,Images,4,2,PyTorch is an optimized deep learning tensor library based on Python and Torch and is mainly used for applications using GPUs and CPUs
Unstructured Data Analysis,Images,4,2,One advantage that PyTorch has over other deep learning frameworks such as TensorFlow and Keras is that it uses dynamic computation graphs and is completely Pythonic
Unstructured Data Analysis,Images,4,2,"It allows scientists, developers, and neural network debuggers to run and test portions of the code in real time, so you don’t have to wait for the entire code to be implemented to check if a part of the code works or not"
Unstructured Data Analysis,Images,4,2,Then we download the dataset we would like to train and predict
Unstructured Data Analysis,Images,4,2,"The dataset we’ll use here is the well-known and widely-used MNIST (Modified National Institute of Standards and Technology) database [7], which is a large database of handwritten digits that is commonly used for training various image processing systems"
Unstructured Data Analysis,Images,4,2,The database is also widely used for training and testing in the field of machine learning
Unstructured Data Analysis,Images,4,2,Figure 2 shows some examples of handwritten digits that we’d like to classify from MNIST
Unstructured Data Analysis,Images,4,2,"Edge detection is a process that aims to capture significant properties of objects in the image, which include discontinuities in the photometrical, geometric, and physical characteristics of objects"
Unstructured Data Analysis,Images,4,2,This information results in variations in the gray level image
Unstructured Data Analysis,Images,4,2,The purpose of edge detection is to localize these variations and to identify the physical phenomena that produce them
Unstructured Data Analysis,Images,4,2,"Edge detection is one of the most important first steps in image processing, and must be efficient and reliable because the validity, efficiency, and possibility of the completion of subsequent processing stages rely on it"
Unstructured Data Analysis,Images,4,2,Edge detection operators are based on the idea that edge information in an image is found by considering the relationship between a pixel and its neighbors
Unstructured Data Analysis,Images,4,2,"If a pixel’s gray-level value is similar to those around it, there is probably not an edge at that point"
Unstructured Data Analysis,Images,4,2,"However, if a pixel has neighbors with widely varying gray levels, it may represent an edge point"
Unstructured Data Analysis,Images,4,2,"Ideally, an edge separates two distinct objects"
Unstructured Data Analysis,Images,4,2,"In practice, apparent edges are caused by changes in color, texture, or by the specific lighting conditions present during the image acquisition process"
Unstructured Data Analysis,Images,4,2,This means that what we refer to as image objects may actually be only parts of the objects in the real world
Unstructured Data Analysis,Images,4,2,The edge detection operators that we cover here represent the various types of operators most commonly used today
Unstructured Data Analysis,Images,4,2,"Many are implemented with kernels or convolution matrices discussed previously, and most are based on discrete approximations to differential operators"
Unstructured Data Analysis,Images,4,2,"Differential operations measure the rate of change in a function (in the same spirit as stochastic gradient descent, discussed earlier)—in this case, the image brightness function"
Unstructured Data Analysis,Images,4,2,A large change in image brightness over a short spatial distance indicates the presence of an edge
Unstructured Data Analysis,Images,4,2,Some edge detection operators return orientation information (i
Unstructured Data Analysis,Images,4,2,", information about the direction of the edge), while others only return information about the existence of an edge at each point"
Unstructured Data Analysis,Images,4,2,"Here, we mention a caveat as well as a taster of what’s to come further on in this topic: With many edge detection operators, noise in the image can create problems"
Unstructured Data Analysis,Images,4,2,"That is why it is best to pre-process the image to eliminate, or at least minimize, noise effects"
Unstructured Data Analysis,Images,4,2,To deal with noise effects we must make trade-offs between the sensitivity and the accuracy of an edge detector
Unstructured Data Analysis,Images,4,2,"For example, if the parameters are adjusted so that the edge detector is very sensitive, it will tend to find many potential edge points that are actually attributable to noise"
Unstructured Data Analysis,Images,4,2,"If we make it less sensitive, it may miss valid edges"
Unstructured Data Analysis,Images,4,2,The parameters that we can vary include the size of the edge detection kernel and the value of the gray-level threshold
Unstructured Data Analysis,Images,4,2,"A larger kernel or a higher gray-level threshold will tend to reduce noise effects, but may result in a loss of valid edge points"
Unstructured Data Analysis,Images,4,2,We’ll discuss image denoising in the next section of this topic
Unstructured Data Analysis,Images,4,2,Physical edges provide important visual information
Unstructured Data Analysis,Images,4,2,"The principal physical edges correspond to significant variations in the reflectance, illumination, orientation, and depth of scene surfaces"
Unstructured Data Analysis,Images,4,2,"Since image intensity is often proportional to scene radiance, physical edges are represented in the image by changes in the intensity function"
Unstructured Data Analysis,Images,4,2,"The most common types of image intensity variations are steps, lines, and junctions"
Unstructured Data Analysis,Images,4,2,Steps are by far the most common type of edge encountered
Unstructured Data Analysis,Images,4,2,This type of edge are caused by various phenomena
Unstructured Data Analysis,Images,4,2,"Some examples of steps are when one object hides another, or when there is a shadow on a surface"
Unstructured Data Analysis,Images,4,2,"It generally occurs between two regions with almost constant, but still different, grey levels"
Unstructured Data Analysis,Images,4,2,The step edge is the point at which the grey level discontinuity occurs
Unstructured Data Analysis,Images,4,2,"In real images, step edges are localized at the inflection points of the image"
Unstructured Data Analysis,Images,4,2,"In fact, the image formation process involves the convolution of the camera point-spread function with the edge profile that is actually corrupted by noise to produce a smoother result"
Unstructured Data Analysis,Images,4,2,"As a consequence, step edges are localized as positive maxima or negative minima of the first-order derivative or as zero-crossings of the second-order derivative"
Unstructured Data Analysis,Images,4,2,Lines come from mutual illumination between objects that are in contact or from thin objects placed against a background
Unstructured Data Analysis,Images,4,2,Lines correspond to local extrema of the image
Unstructured Data Analysis,Images,4,2,"They are localized as zero-crossings of the first derivative, or local maxima of the Laplacian (more on this later), or local maxima of the grey level variance of the smoothed image"
Unstructured Data Analysis,Images,4,2,"This type of edge is successfully used in remote sensing images, for instance, to detect roads and rivers"
Unstructured Data Analysis,Images,4,2,A physical corner or junction is formed when at least two physical edges meet
Unstructured Data Analysis,Images,4,2,"There are additional circumstances that may create corners in the image, for example, illumination effects or occlusions"
Unstructured Data Analysis,Images,4,2,"The junction can be localized in various ways, such as by a point with high curvature, or a point with great variation in gradient direction, or a zero-crossing of the Laplacian with high curvature"
Unstructured Data Analysis,Images,4,2,"The term edge, as commonly used, encompasses all of these types of edges, but the majority of existing edge detection algorithms are adapted to step edges, which are the most common"
Unstructured Data Analysis,Images,4,2,Gradient operators are based on first or second derivatives of the gray-level function as an edge detector
Unstructured Data Analysis,Images,4,2,Recall from calculus (and as discussed previously) that the derivative measures the rate of change of a line or the slope of the line
Unstructured Data Analysis,Images,4,2,"When the gray level is constant, the first derivative is zero, and when it is linear it is equal to the slope of the line"
Unstructured Data Analysis,Images,4,2,"With the following operators we will see that this is approximated with a difference operator, similar to the methods used in the definition of the derivative that you’ve seen from calculus"
Unstructured Data Analysis,Images,4,2,The second derivative measures the rate of change of the derivative
Unstructured Data Analysis,Images,4,2,"It is positive at the change on the dark side of the edge, negative at the change on the light side, and zero elsewhere"
Unstructured Data Analysis,Images,4,2,"The Robinson compass masks are used in a manner similar to the Kirsch masks, but are easier to implement, as they rely only on coefficients of 0, 1, and 2, and are symmetrical about their directional axis—the axis with the zeros that corresponds to the line direction"
Unstructured Data Analysis,Images,4,2,We only need to compute the results on four of the masks; the results from the other four can be obtained by negating the results from the first four
Unstructured Data Analysis,Images,4,2,Another class of operator that is differential in nature but offers more flexibility than the gradient operators just discussed is the Laplacian operators
Unstructured Data Analysis,Images,4,2,"To get an intuition on how these operators work, we first look at its definition"
Unstructured Data Analysis,Images,4,2,"In evaluating the performance of processing techniques, we consider both objective and subjective evaluations"
Unstructured Data Analysis,Images,4,2,"The objective metric allows us to compare different techniques using fixed analytical methods, whereas the subjective methods may have unpredictable results"
Unstructured Data Analysis,Images,4,2,"However, for many image processing applications, the subjective measures in fact tend to be the most useful"
Unstructured Data Analysis,Images,4,2,"To develop a performance metric for edge detection operators, we need to first determine what constitutes success"
Unstructured Data Analysis,Images,4,2,"For example, the Canny algorithm was developed considering three important edge detection success criteria"
Unstructured Data Analysis,Images,4,2,"These criteria correspond nicely to Pratt’s Figure of Merit (FOM) [1], which considers the types of errors that can occur with edge detection methods"
Unstructured Data Analysis,Images,4,2,The types of errors are missing valid edge points; classifying noise pulses as valid edge points; and smearing of edges
Unstructured Data Analysis,Images,4,2,"If these errors do not occur, then edge detection is deemed successful"
Unstructured Data Analysis,Images,4,2,"The distance, d, can be defined in more than one way and typically depends on the connectivity definition used"
Unstructured Data Analysis,Images,4,2,"Some commonly used distances are the city-block distance, which is based on 4-connectivity and where movements are only horizontal and vertical (see Definition 2"
Unstructured Data Analysis,Images,4,2,"9 above); chessboard distance, which is based on 8-connectivity, allowing for additional diagonal movements (see Definition 2"
Unstructured Data Analysis,Images,4,2,"11); and the Euclidean distance, which is based on actual physical distance (see Definition 2"
Unstructured Data Analysis,Images,4,2,"Image denoising is to remove noise from a noisy image, so as to restore the true image"
Unstructured Data Analysis,Images,4,2,"However, since noise, edge, and texture are high frequency components, it is difficult to distinguish them in the process of denoising and the denoised images could inevitably lose some details"
Unstructured Data Analysis,Images,4,2,"Overall, recovering meaningful information from noisy images in the process of noise removal to obtain high quality images is an important problem and an important image processing task"
Unstructured Data Analysis,Images,4,2,The purpose of noise reduction is to decrease the noise in natural images while minimizing the loss of original features and improving the signal-to-noise ratio (SNR)
Unstructured Data Analysis,Images,4,2,"Under the influence of noise, the gray scale of image pixels will change"
Unstructured Data Analysis,Images,4,2,Noise is often random and the effect of random noise on a particular image is therefore uncertain
Unstructured Data Analysis,Images,4,2,"In many cases, the noise has statistical characteristics which allows us to describe noise as a statistical process"
Unstructured Data Analysis,Images,4,2,"Noise can be superimposed on the original image, which is then known as additive noise"
Unstructured Data Analysis,Images,4,2,"It can also be multiplied with the original image, which is then called multiplicative noise"
Unstructured Data Analysis,Images,4,2,"If the gray level of the noise itself is regarded as a random variable, its distribution can be described by a probability density function (pdf)"
Unstructured Data Analysis,Images,4,2,Here are some important noise pdfs
Unstructured Data Analysis,Images,4,2,Noise from images may be removed by applying a filter
Unstructured Data Analysis,Images,4,2,"There exist many filters, which are largely classified by their approaches, which is largely driven by the type of noise and where the filter is applied"
Unstructured Data Analysis,Images,4,2,"In addition, there are filters that handle periodic noise, which refers to the regular repetition of noise in the image"
Unstructured Data Analysis,Images,4,2,This kind of noise is often caused by electrical interference when collecting images and it varies with spatial position
Unstructured Data Analysis,Images,4,2,"Because periodic noise has a specific frequency, there is a class of frequency domain filters which is often used to eliminate it"
Unstructured Data Analysis,Images,4,2,"This kind of noise will not be covered in this topic since to fully appreciate its technicality and understand the philosophy behind the approach, knowledge of Fourier analysis is required, which is beyond the scope of this module"
Unstructured Data Analysis,Images,4,2,Order Statistical Filters
Unstructured Data Analysis,Images,4,2,Order statistical filters perform filtering by sorting the gray values in images; they are another common type of spatial noise filter and are nonlinear filters
Unstructured Data Analysis,Images,4,2,Here are the most common order statistical filters
Unstructured Data Analysis,Images,4,2,"Another filter that is very similar in spirit to the mean filters and median filter just presented, though neither a mean nor an order statistical filter, is the mode filter"
Unstructured Data Analysis,Images,4,2,"Recall that in statistics, in addition to the mean and median, the mode value is also a common statistical summary and represents the most likely value in a distribution: it is the value that is the most frequently occurring among observations"
Unstructured Data Analysis,Images,4,2,Using the mode filter can not only eliminate noise (especially impulse noise) but also sharpen the edge of the object
Unstructured Data Analysis,Images,4,2,"This is because, in the neighborhood close to the edge, the mode filter will move the mode closer to the center of the edge, which makes the edge sharper"
Unstructured Data Analysis,Images,4,2,More precisely (no pun intended
Unstructured Data Analysis,Images,4,2,"!), the pixels on the background side of any edge mainly have background gray values, so the output of the mode filter will be the gray value of the background; while the pixels on the foreground side of any edge will mainly have foreground gray values, so the output of the mode filter will be the gray value of the foreground"
Unstructured Data Analysis,Images,4,2,"In this way, at a certain point on the edge, the main peak of the local grayscale distribution changes from the background to the foreground or from the foreground to the background, which tends to enhance the edge"
Unstructured Data Analysis,Images,4,2,"This procedure is different from a filter that works by average, which is an operation that blurs the edges"
Unstructured Data Analysis,Images,4,2,"Filtering by averaging will, on the other hand, produce an edge profile with a mixture of background and foreground gray levels, which will reduce the local contrast between these two regions"
Unstructured Data Analysis,Images,4,2,"The grayscale distribution in a region can be represented by the histogram of the region, and the mean, median, and mode are all closely related to the histogram, as in usual statistics"
Unstructured Data Analysis,Images,4,2,The mean value of the histogram of a region gives the mean gray value of the region
Unstructured Data Analysis,Images,4,2,The median value of the histogram of a region also gives the median gray value of the region
Unstructured Data Analysis,Images,4,2,The mode value of the histogram of a region is the gray value with the largest statistical value
Unstructured Data Analysis,Images,4,2,"If the histogram is symmetric and there is only one peak, then the mean, median, and mode are all the same"
Unstructured Data Analysis,Images,4,2,"If there is only one peak in the histogram, but the left and right sides of the histogram are asymmetric, then the mode value corresponds to that peak, and the median is always closer to the mode value than the mean"
Unstructured Data Analysis,Images,4,2,"Direct detection of the mode value, however, may be difficult due to the influence of noise, resulting in inaccurate detection"
Unstructured Data Analysis,Images,4,2,"If the median value has been determined, the median position can be used to then determine the mode position more accurately"
Unstructured Data Analysis,Images,4,2,The method of filtering by truncating the median can be used here
Unstructured Data Analysis,Images,4,2,"This proceeds in the following way: first, according to the median value, cut the longer part of the tail to make it the same length as the untruncated part, then calculate the median value of the remaining part"
Unstructured Data Analysis,Images,4,2,"Then crop this part as before and repeat, so that the iteration gradually approaches the mode position"
Unstructured Data Analysis,Images,4,2,"Here, the relationship that the median position is closer to the mode position than the mean position is used"
Unstructured Data Analysis,Images,4,2,"Linear filters, such as most of the mean filters discussed previously, can effectively eliminate Gaussian noise and uniformly distributed noise, but their elimination of salt and pepper noise is very poor"
Unstructured Data Analysis,Images,4,2,"The median filter can effectively eliminate impulse noise, such as salt and pepper noise, and will not blur the image too much; however, the effect of eliminating Gaussian noise, on the other hand, is not very good"
Unstructured Data Analysis,Images,4,2,"When an image is affected by both salt and pepper noise and Gaussian noise, it can be divided into two sets after detecting the pixel set affected by the salt and pepper noise as described above"
Unstructured Data Analysis,Images,4,2,"One set is only affected by Gaussian noise, while the other set is affected by both Gaussian noise and salt and pepper noise"
Unstructured Data Analysis,Images,4,2,The influence of Gaussian noise on these pixels affected by salt and pepper noise can be ignored since the grayscale of the pixels affected by salt and pepper noise takes the two extreme values of the gray scale of the image
Unstructured Data Analysis,Images,4,2,"To eliminate salt and pepper noise in affected pixels, the surrounding pixels that are not affected by salt and pepper noise can be used for interpolation"
Unstructured Data Analysis,Images,4,2,A filter designed to to eliminate Gaussian noise from pixels that are not affected by salt and pepper noise can be applied (many predefined functions in packages use an adaptive Wiener filter)
Unstructured Data Analysis,Images,4,2,"Finally, the two results are combined to provide the output denoised image"
Unstructured Data Analysis,Images,4,2,Image segmentation is a process that divides images into multiple regions or segments in such a way that its representation is easier to analyze so that the interpretation is more meaningful
Unstructured Data Analysis,Images,4,2,"It classifies or clusters the image into several parts (regions) according to the feature of image, for example, the pixel value or the frequency response, which makes it relatable to other tasks, such as edge detection"
Unstructured Data Analysis,Images,4,2,"Region-based methods mainly rely on the assumption that the neighboring pixels within one region have similar value, so the procedure compares one pixel with its neighbors"
Unstructured Data Analysis,Images,4,2,"If a similarity criterion is satisfied between this pixel and its neighbors, the pixel is set belong to the cluster, along with one or more of its neighbors"
Unstructured Data Analysis,Images,4,2,"Here, predefining the similarity criterion is crucial, and the results are influenced by noise in all instances"
Unstructured Data Analysis,Images,4,2,The seeded region growing (SRG) algorithm is one of the simplest region-based segmentation methods
Unstructured Data Analysis,Images,4,2,It segments an image by examining the neighboring pixels of a set of seed points (pixels) and determines whether they can be classified to the cluster of the seed point or not
Unstructured Data Analysis,Images,4,2,The algorithm proceeds as follows
Unstructured Data Analysis,Images,4,2,"Each of the segmentation regions of the SRG algorithm has high color similarity and no fragmentary issues, by design"
Unstructured Data Analysis,Images,4,2,"However, it still has three important drawbacks"
Unstructured Data Analysis,Images,4,2,"The first is that different sets of initial seed points lead to different segmentation results, which negatively affects the stability of segmentation results"
Unstructured Data Analysis,Images,4,2,"Furthermore, the number of initial seed points is an important issue because it can vary from image to image, depending on the image type, application, research problem, source, etc"
Unstructured Data Analysis,Images,4,2,"Finally, the SRG is a time-consuming algorithm, which, despite the initial seed issues just discussed, turns out to be the most serious problem of SRG"
Unstructured Data Analysis,Images,4,2,The unseeded region growing (URG) algorithm is a derivative of the SRG algorithm
Unstructured Data Analysis,Images,4,2,This algorithm bypasses the initial seeding drawback of the SRG and its main feature is that no explicit seed selection is necessary; the seeds are generated automatically
Unstructured Data Analysis,Images,4,2,It is therefore a fully automatic segmentation procedure with the added benefit of the robustness of region-based segmentation
Unstructured Data Analysis,Images,4,2,The algorithm proceeds as follows
Unstructured Data Analysis,Images,4,2,The main goal of region splitting and merging is to determine the homogeneity of the image
Unstructured Data Analysis,Images,4,2,Let R represent the entire image region and decide a predicate Q
Unstructured Data Analysis,Images,4,2,"The purpose is that if Q(R) = FALSE, we divide the image R into quadrants"
Unstructured Data Analysis,Images,4,2,"If Q is FALSE for any quadrant, we subdivide that quadrant into subquadrants, and so on"
Unstructured Data Analysis,Images,4,2,"Until that, for any region Ri, Q(Ri) = TRUE"
Unstructured Data Analysis,Images,4,2,"After the process of splitting, merging process is to merge two adjacent regions Rj and Rk if Q(Rj ∪ Rk) = TRUE"
Unstructured Data Analysis,Images,4,2,The algorithm can be summarized as follows
Unstructured Data Analysis,Images,4,2,"The advantages of region splitting and merging are that the image can be split progressively according to resolution; and the image can be split using the user-specified criteria, such as mean or variance of segment pixel value"
Unstructured Data Analysis,Images,4,2,"In addition, the merging criteria can be set differently to the splitting criteria"
Unstructured Data Analysis,Images,4,2,"On the other hand, region splitting and merging tends to produce “blocky” segments, which can be reduced by splitting into further components, but this increases the computation time"
Unstructured Data Analysis,Images,4,2,Thresholding segmentation techniques are popular since their nature is intuitive and they are often simple and fast to implement
Unstructured Data Analysis,Images,4,2,"Given an image, suppose that object and background pixels have intensity values that group into two dominant modes"
Unstructured Data Analysis,Images,4,2,"To extract the objects from the background, simply select a threshold t that separates these modes: any point (x,y) in the image at which f(x,y) > t is then an object point"
Unstructured Data Analysis,Images,4,2,"Otherwise, it is a background point"
Unstructured Data Analysis,Images,4,2,"When the value of t changes over an image, the segmentation is a variable thresholding process"
Unstructured Data Analysis,Images,4,2,"The terms local or regional thresholding are sometimes used to describe variable thresholding where the value of t at any point (x,y) in an image depends on certain properties of a neighborhood of (x,y), e"
Unstructured Data Analysis,Images,4,2,g
Unstructured Data Analysis,Images,4,2,", the average intensity of the pixels in the neighborhood"
Unstructured Data Analysis,Images,4,2,"If t depends on the spatial coordinates (x,y), then variable thresholding is referred to as dynamic or adaptive thresholding"
Unstructured Data Analysis,Networks,4,2,Networks capture information on relationships and are used to model interactions
Unstructured Data Analysis,Networks,4,2,"Every day, you engage in a network and contribute to network data when you use your cell phone, log on to the internet, engage in social media, take public transport, etc"
Unstructured Data Analysis,Networks,4,2,They are one of the most fundamental data structures that we encounter on a daily basis
Unstructured Data Analysis,Networks,4,2,"Some of the most important and pressing data challenges entail working with networks—for example, identifying and predicting cybersecurity threats; understanding processes in biochemical reaction networks to develop therapies; identifying relevant targets in social media for effective advertisement, etc"
Unstructured Data Analysis,Networks,4,2,"Networks, as data structures, require special care and attention for data analysis and processing, because they aren’t vector spaces and aren’t metric spaces"
Unstructured Data Analysis,Networks,4,2,"For this reason, we can think of them as belonging to the domain of unstructured data, since unconventional approaches are fundamental to deal with them"
Unstructured Data Analysis,Networks,4,2,"In this topic, we’ll start with the basic characterizations of networks and approaches to deal with them"
Unstructured Data Analysis,Networks,4,2,"These include methods that vectorize networks so that we may use classical statistical vector- and metric-space based approaches, as well as techniques that deal with the network structure directly"
Unstructured Data Analysis,Networks,4,2,"Throughout this topic, we use the expressions “network” and “graph” interchangeably"
Unstructured Data Analysis,Networks,4,2,"Typically, the term “graph” is used when discussing more theoretical aspects, since graphs are a very classical mathematical object that were studied independently of their capacity to capture data in the sense that we are interested in; they are interesting mathematical objects in their own right and have a rich history and foundation in pure mathematics as well"
Unstructured Data Analysis,Networks,4,2,"A graph G is a finite nonempty set of objects, called vertices (the singular is vertex), together with a (possibly empty) set of unordered pairs of distinct vertices, called edges"
Unstructured Data Analysis,Networks,4,2,"The set of vertices of a graph G is called the vertex set of G, denoted by V (G), and the set of edges is called the edge set of G, denoted by E(G)"
Unstructured Data Analysis,Networks,4,2,"The edge e = {u,v} is said to join the vertices u and v"
Unstructured Data Analysis,Networks,4,2,"If e = {u,v} (or simply e = uv) is an edge of G, then u and v are adjacent vertices, while u and e are said to be incident, as are v and e"
Unstructured Data Analysis,Networks,4,2,"Furthermore, if e1 and e2 are distinct edges of G incident with a common vertex, then e1 and e2 are adjacent edges"
Unstructured Data Analysis,Networks,4,2,"For convenience, therefore, we denote an edge by uv or vu rather than by {u, v}"
Unstructured Data Analysis,Networks,4,2,"The cardinality of the vertex set of a graph G is called the order of G and is denoted by n(G) (or sometimes just simply by n when there is no ambiguity as to which graph we are studying), while the cardinality of its edge set is the size of G, denoted by m(G) or m"
Unstructured Data Analysis,Networks,4,2,"An (n, m) graph has order n and size m"
Unstructured Data Analysis,Networks,4,2,The graph of order n = 1 is called the trivial graph
Unstructured Data Analysis,Networks,4,2,A nontrivial graph has at least two vertices
Unstructured Data Analysis,Networks,4,2,Let v be a vertex of a graph G
Unstructured Data Analysis,Networks,4,2,The degree of v is the number of edges of G incident with v
Unstructured Data Analysis,Networks,4,2,"The degree of v is denoted by degG(v), or simply deg(v) if G is clear from the context"
Unstructured Data Analysis,Networks,4,2,"The minimum degree of G is the minimum degree among all the vertices of G and is denoted by δ(G), while the maximum degree of G is the maximum degree among all the vertices of G and is denoted by ∆(G)"
Unstructured Data Analysis,Networks,4,2,The open neighborhood of a vertex v of G is the set
Unstructured Data Analysis,Networks,4,2,There are two data structures that are often used to represent graphs: the adjacency list and the adjacency matrix
Unstructured Data Analysis,Networks,4,2,"In both of these representations, if vertices or edges are used to store data, then we assume there is some way of mapping vertices and edges to the data that are associated with them"
Unstructured Data Analysis,Networks,4,2,"For example, we may use a lookup structure to store objects using vertex or edge names as keys, or we may represent edges or vertices as multiple-field objects and store the data associated with the edges or vertices in these fields"
Unstructured Data Analysis,Networks,4,2,The main difference between these two graph representations are that we may get different time performances for various graph operations depending on how our graph is represented; this will be studied in further detail in this topic
Unstructured Data Analysis,Networks,4,2,"For a graph G with n vertices and m edges, an adjacency list representation uses O(n + m) space, whereas an adjacency matrix representation uses O(n2) space"
Unstructured Data Analysis,Networks,4,2,"A collection V of n vertices: This collection could be a set, list, or array, or it could even be defined implicitly as simply the integers from 1 to n"
Unstructured Data Analysis,Networks,4,2,"If vertices can “store” data, there also needs to be some way to map each vertex v to the data associated with v"
Unstructured Data Analysis,Networks,4,2,"A collection E of m edges: In other words, a collection of pairs of vertices"
Unstructured Data Analysis,Networks,4,2,"This collection could be a set, list, or array, or it could even be defined implicitly by the pairs of vertices that are determined by adjacency lists"
Unstructured Data Analysis,Networks,4,2,"If edges can “store” data, there also needs to be some way to map each edge e to the data associated with e"
Unstructured Data Analysis,Networks,4,2,"An adjacency list for v: For each vertex v in V , we store a list, called the adjacency list for v, that represents all the edges incident on v"
Unstructured Data Analysis,Networks,4,2,"This is implemented either as a list of references to each vertex w such that (v, w) is an edge in E, or it is implemented as a list of references to each edge e that is incident on v"
Unstructured Data Analysis,Networks,4,2,"If G is a directed graph, then the adjacency list for v is typically divided into two parts: one representing the incoming edges for v and one representing the outgoing edges for v"
Unstructured Data Analysis,Networks,4,2,"In the adjacency matrix representation of a graph G, we represent the edges in G using (a two- dimensional array) matrix, A"
Unstructured Data Analysis,Networks,4,2,This representation allows us to determine adjacencies between pairs of vertices in constant time
Unstructured Data Analysis,Networks,4,2,"As we shall see, the trade-off in achieving this speedup is that the space usage for representing a graph of n vertices is larger and of the order O(n2), even if the graph has few edges"
Unstructured Data Analysis,Networks,4,2,"Using an adjacency matrix A, we can determine whether two vertices v and w are adjacent in O(1) time"
Unstructured Data Analysis,Networks,4,2,"We can achieve this performance by accessing the vertices v and w to determine their respective indices i and j, then testing whether the cell A[i, j] is zero or not"
Unstructured Data Analysis,Networks,4,2,"This performance achievement is traded off by an increase in the space usage, however (which is O(n2)), as well as in the running time of some other graph operations"
Unstructured Data Analysis,Networks,4,2,"For example, listing out the incident edges or adjacent vertices for a vertex v now requires that we examine an entire row or column of the array A representing the graph, which takes O(n) time"
Unstructured Data Analysis,Networks,4,2,"As mentioned above and as can be seen from the mathematical definitions of graphs, they encode relationships, but they are not a vector space nor a metric space"
Unstructured Data Analysis,Networks,4,2,Graph embedding provides an effective yet efficient way to overcome this problem; it is a method to vectorize graphs
Unstructured Data Analysis,Networks,4,2,"Specifically, graph embedding converts a graph into a low dimensional space in which the graph information is preserved"
Unstructured Data Analysis,Networks,4,2,"By representing a graph as a (or a set of) low dimensional vec- tor(s), graph algorithms can then be computed efficiently"
Unstructured Data Analysis,Networks,4,2,There are different types of graphs (e
Unstructured Data Analysis,Networks,4,2,", homogeneous graphs, heterogeneous graphs, attribute graphs, etc"
Unstructured Data Analysis,Networks,4,2,"), so the input to a graph embed- ding procedure varies in different scenarios"
Unstructured Data Analysis,Networks,4,2,"The output of graph embedding is a low-dimensional vector representing a part of the graph (or the whole graph), i"
Unstructured Data Analysis,Networks,4,2,", according to different needs, we may represent a node/edge/substructure/whole-graph as a low dimensional vector"
Unstructured Data Analysis,Networks,4,2,"With the encoder–decoder framework, the graph embedding problem breaks down into two key operations: first, an encoder model maps each node in the graph into a low-dimensional vector or embedding"
Unstructured Data Analysis,Networks,4,2,"Next, a decoder model takes the low-dimensional node embeddings and uses them to reconstruct information about each node’s neighborhood in the original graph"
Unstructured Data Analysis,Networks,4,2,Optimizing An Encoder–Decoder Model
Unstructured Data Analysis,Networks,4,2,"In the deconstruction–reconstruction framework of the encoder–decoder approach, we would like for the reconstruction to retain as much information about the original graph"
Unstructured Data Analysis,Networks,4,2,This is normally captured mathematically by minimizing an empirical reconstruction loss L over a set of training node pairs D
Unstructured Data Analysis,Networks,4,2,Another perspective to the encoder–decoder approach just discussed is to view it as a form of ma- trix factorization: the challenge of decoding local neighborhood structure from a node’s embed- ding is essentially reconstructing entries in the graph adjacency matrix
Unstructured Data Analysis,Networks,4,2,By the first property above in Theorem 3
Unstructured Data Analysis,Networks,4,2,"This task can be interpreted mathematically as matrix factorization where the aim is to learn a low-dimensional approximation of a node–node similarity matrix S, where S generalizes the adjacency matrix and captures some user-defined notion of node–node similarity"
Unstructured Data Analysis,Networks,4,2,"2, the right-hand side is a measurement of the total variation of a function on the graph"
Unstructured Data Analysis,Networks,4,2,"Therefore to find the function with minimal variation, we need to find the eigenvectors of the Laplacian matrix"
Unstructured Data Analysis,Networks,4,2,This gives the following pipeline to perform spectral clustering:
Unstructured Data Analysis,Networks,4,2,"So far, we’ve seen deterministic measures of node similarity, where S is taken as some polynomial function of the adjacency matrix"
Unstructured Data Analysis,Networks,4,2,"The node embeddings are optimized so that z⊤u zv ≈ S[u, v]"
Unstructured Data Analysis,Networks,4,2,More recent methods that from the class of random walk embeddings build on this approach but instead use stochastic measures of neighborhood overlap
Unstructured Data Analysis,Networks,4,2,"More specifically, node embeddings are optimized so that two nodes have similar embeddings if they tend to co-occur on short random walks over the graph"
Unstructured Data Analysis,Networks,4,2,Two popular methods—DeepWalk and node2vec—use a shallow embedding approach and an inner-product decoder but define different notions of node similarity and neighborhood recon- struction
Unstructured Data Analysis,Networks,4,2,"Instead of directly reconstructing the adjacency matrix A (or, more generally, some deterministic function of A), these approaches optimize embeddings to encode the statistics of random walks"
Unstructured Data Analysis,Networks,4,2,"Here, we use σ to denote the logistic function, Pn(V) to denote a distribution over the set of nodes V, and we assume that γ > 0 is a hyperparameter"
Unstructured Data Analysis,Networks,4,2,"In practice, Pn(V) is often defined to be a uniform distribution, and the expectation is approximated using Monte Carlo sampling"
Unstructured Data Analysis,Networks,4,2,The node2vec approach also distinguishes itself from the earlier DeepWalk algorithm by allowing for a more flexible definition of random walks
Unstructured Data Analysis,Networks,4,2,"In particular, whereas DeepWalk simply employs uniformly random walks to define pG,T (v | u), the node2vec approach introduces hyperparameters that allow the random walk probabilities to smoothly interpolate between walks that are more akin to breadth-first search or depth-first search over the graph"
Unstructured Data Analysis,Networks,4,2,"The LINE approach is another stochastic model which does not explicitly leverage random walks, but it shares conceptual motivations with DeepWalk and node2vec"
Unstructured Data Analysis,Networks,4,2,The basic idea in LINE is to combine two encoder–decoder objectives
Unstructured Data Analysis,Networks,4,2,"Despite their conceptual differences, random walk methods are actually closely related to matrix factorization approaches."
Unstructured Data Analysis,Networks,4,2,Embeddings learned by DeepWalk are thus closely related to the spectral clustering embed- dings
Unstructured Data Analysis,Networks,4,2,The key difference is that the DeepWalk embeddings control the influence of different
Unstructured Data Analysis,Networks,4,2,"A natural and important question in dealing with network data is how to “get in the thick of the network”—specifically, how do we traverse from one node to another in a network? How do we search within a network? These are difficult problems since in general, there is no strict ordering to the nodes and edges in a graph: remember, although we index the vertices and edges (e"
Unstructured Data Analysis,Networks,4,2,g
Unstructured Data Analysis,Networks,4,2,", v1, v2, etc"
Unstructured Data Analysis,Networks,4,2,"), these are just dummy variables"
Unstructured Data Analysis,Networks,4,2,"This means that in the most general sense, networks are not metric spaces and we cannot define a location or address to nodes, even though we can represent a graph as a matrix"
Unstructured Data Analysis,Networks,4,2,"This is a key difference between matrix representations of graphs and images, which we saw in the previous topic (recall that pixels do have locations)"
Unstructured Data Analysis,Networks,4,2,These characteristics of a lack of ordering and metric space structure become especially pertinent and intimidating when we want to study very large networks
Unstructured Data Analysis,Networks,4,2,"Search algorithms are the answer to one of the most fundamental problems when working with networks: traversing the edges and vertices, which intuitively amounts to traveling over a graph"
Unstructured Data Analysis,Networks,4,2,"For example, a web spider, or crawler, collects data for a search engine; to do this, it must explore a large graph of hypertext documents by examining its vertices, which are the documents, and its edges, which are the hyperlinks between documents"
Unstructured Data Analysis,Networks,4,2,"Specifically, a traversal is a systematic procedure for exploring a graph by examining all of its vertices and edges"
Unstructured Data Analysis,Networks,4,2,"A traversal is efficient if it visits all the vertices and edges in time proportional to their number—that is, in linear time"
Unstructured Data Analysis,Networks,4,2,"First, let’s look at undirected graphs"
Unstructured Data Analysis,Networks,4,2,Depth-first search (DFS) is a traversal algorithm that is useful for performing a number of computations on graphs; including finding a path from one vertex to another; determining whether a graph is connected; and computing a spanning tree of a connected graph
Unstructured Data Analysis,Networks,4,2,A spanning tree is a subgraph of a graph that hits every vertex in the graph and is a tree
Unstructured Data Analysis,Networks,4,2,Depth-first search in an undirected graph G applies the backtracking technique; the way it works is analogous to wandering in a labyrinth with a string and a can of paint without getting lost
Unstructured Data Analysis,Networks,4,2,"We begin at a starting vertex s in G, which we initialize by fixing one end of our string to s and marking (painting) s as “explored"
Unstructured Data Analysis,Networks,4,2,” The vertex s is now our “current” vertex; call our current vertex v
Unstructured Data Analysis,Networks,4,2,"We then traverse G by considering an arbitrary edge (v, w) incident to the current vertex, v"
Unstructured Data Analysis,Networks,4,2,"If the edge (v, w) leads us to an already explored (marked or painted) vertex w, then we backtrack to vertex v"
Unstructured Data Analysis,Networks,4,2,"If, on the other hand, (v, w) leads to an unexplored vertex w, then we unroll our string, and go to w"
Unstructured Data Analysis,Networks,4,2,"We then mark w as “explored” and make it the current vertex, and repeat the same process"
Unstructured Data Analysis,Networks,4,2,Another important traversal algorithm is the breadth-first search (BFS) traversal algorithm
Unstructured Data Analysis,Networks,4,2,"Like DFS, BFS traverses a connected component of a graph"
Unstructured Data Analysis,Networks,4,2,"Instead of searching recursively, however, BFS proceeds in rounds and subdivides the vertices into levels, which represent the minimum number of edges from the start vertex to each vertex"
Unstructured Data Analysis,Networks,4,2,This process of graph searching defines a useful spanning tree
Unstructured Data Analysis,Networks,4,2,BFS starts at a given start vertex s which is at level 0 and defines the “anchor” for our string
Unstructured Data Analysis,Networks,4,2,"In the first round, we explore all the vertices we can reach in one edge, marking each as explored"
Unstructured Data Analysis,Networks,4,2,These vertices are placed into level 1
Unstructured Data Analysis,Networks,4,2,"In the second round, we explore all the vertices that can be reached in two edges from the start vertex"
Unstructured Data Analysis,Networks,4,2,"These new vertices, which are adjacent to level 1 vertices and not previously assigned to a level, are placed into level 2"
Unstructured Data Analysis,Networks,4,2,The procedure continues in this manner placing vertices into higher levels
Unstructured Data Analysis,Networks,4,2,"In a road network, the interconnection structure of a set of roads is modeled as a graph whose vertices are intersections and dead ends within the set of roads, and edges are defined by segments of road that exists between pairs of such vertices"
Unstructured Data Analysis,Networks,4,2,"A navigation system, such as Google Maps or Waze, aims to find the shortest path that exists between two vertices in such a road network (ignoring for now the presence of other vehicles on the road and the effect of traffic)"
Unstructured Data Analysis,Networks,4,2,A weighted graph is a graph that has a numeric label w(e) associated with each edge e called the weight of edge e.
Unstructured Data Analysis,Networks,4,2,"Edge weights can be integers, rational numbers, or real numbers, representing a concept such as distance, connection costs, or affinity"
Unstructured Data Analysis,Networks,4,2,"Formally, we work in the following context. Let G be a weighted graph. The length (or weight) of a path P made up of connected edges in G is the sum of the weights of the edges of P."
Unstructured Data Analysis,Networks,4,2,Recall that the special case of computing a shortest path when all weights are 1 was solved with the BFS traversal algorithm presented in Section 4
Unstructured Data Analysis,Networks,4,2,"There is an interesting approach for solving this single-source shortest path problem based on a greedy method, which is known as Dijkstra’s algorithm"
Unstructured Data Analysis,Networks,4,2,Greedy algorithms choose the best option at any given run; they do not consider past iterations and do not take into account whether this option is optimal at the end of the run of the algorithm
Unstructured Data Analysis,Networks,4,2,"In other words, the cost for going from vi to vj using vertices numbered 1 through k is equal to the shorter of two possible paths"
Unstructured Data Analysis,Networks,4,2,The first path is simply the shortest path from vi to vj using vertices numbered 1 through k − 1
Unstructured Data Analysis,Networks,4,2,The second path is the sum of the costs of the shortest path from vi to vk using vertices numbered 1 through k − 1 and the shortest path from vk to vj using vertices numbered 1 through k − 1
Unstructured Data Analysis,Networks,4,2,"In what we have seen so far, we touched on the notions of spanning trees and minimum spanning trees a couple of times; the algorithms we discussed have hinted at their usefulness"
Unstructured Data Analysis,Networks,4,2,"Let’s now take a closer look at these objects, why they’re important and two ways of computing them"
Unstructured Data Analysis,Networks,4,2,We can consider the scenario of a telecommunications company wishing to lay down cables in a new neighborhood
Unstructured Data Analysis,Networks,4,2,If it is constrained to bury the cable only along certain paths (e
Unstructured Data Analysis,Networks,4,2,", roads), then there would be a graph containing the points (e"
Unstructured Data Analysis,Networks,4,2,", houses) connected by those paths"
Unstructured Data Analysis,Networks,4,2,Some of the paths might be more expensive: they might be longer or require the cable to be buried deeper
Unstructured Data Analysis,Networks,4,2,These paths would be represented by edges with larger weights
Unstructured Data Analysis,Networks,4,2,Currency is an acceptable unit for edge weights in this case—there is no requirement for edge lengths to obey normal rules of geometry such as the triangle inequality
Unstructured Data Analysis,Networks,4,2,A spanning tree for that graph would be a subset of those paths that has no cycles but still connects every house
Unstructured Data Analysis,Networks,4,2,Note that spanning trees don’t have to be unique; there might be several possible spanning trees
Unstructured Data Analysis,Networks,4,2,"A minimum spanning tree would be one with the lowest total cost, representing the least expensive path for laying the cables"
Unstructured Data Analysis,Networks,4,2,Kruskal’s algorithm builds minimum spanning trees in clusters
Unstructured Data Analysis,Networks,4,2,"Initially, each vertex is in its own cluster all by itself"
Unstructured Data Analysis,Networks,4,2,"The algorithm then considers each edge in turn, ordered by increasing weight"
Unstructured Data Analysis,Networks,4,2,"If an edge e connects two different clusters, then e is added to the set of edges of the minimum spanning tree"
Unstructured Data Analysis,Networks,4,2,The two clusters connected by e are merged into a single cluster
Unstructured Data Analysis,Networks,4,2,"If, on the other hand, e connects two vertices that are already in the same cluster, then e is discarded"
Unstructured Data Analysis,Networks,4,2,"Once the algorithm has added enough edges to form a spanning tree, it terminates and outputs this tree as the minimum spanning tree"
Unstructured Data Analysis,Networks,4,2,The pseudocode for Kruskal’s algorithm to solve the MST problem is given in Algorithm 7
Unstructured Data Analysis,Networks,4,2,"The Prim–Jarník algorithm grows a minimum spanning tree from a single cluster starting from some “root” vertex, v"
Unstructured Data Analysis,Networks,4,2,The main idea is similar to that of Dijkstra’s algorithm
Unstructured Data Analysis,Networks,4,2,"We begin with some vertex v, defining the initial “cloud” of vertices C"
Unstructured Data Analysis,Networks,4,2,"Then, at each iteration, we choose a minimum weight edge e = (v, u) that connects a vertex v in the cloud C to a vertex u outside of C"
Unstructured Data Analysis,Networks,4,2,The vertex u is then absorbed into the cloud C and the process is repeated until a spanning tree is formed
Unstructured Data Analysis,Networks,4,2,"So far, we have seen a variety of algorithms that carry out certain procedures or compute certain things on graphs"
Unstructured Data Analysis,Networks,4,2,How do we know that the algorithm is really doing what it’s supposed to do? This is the question that we want to validate and it is done by proving correctness
Unstructured Data Analysis,Networks,4,2,"To be completely rigorous, all algorithms (to do anything, not just graph search) need a proof that it does what we claim it does, i"
Unstructured Data Analysis,Networks,4,2,", that it returns the correct result"
Unstructured Data Analysis,Networks,4,2,"Networks are the fundamental model to describe, study, and understand complex systems in many scientific disciplines"
Unstructured Data Analysis,Networks,4,2,"One of the most important features in complex systems that networks are able to capture and describe is the presence of communities; in networks, these correspond to the presence of network clusters, i"
Unstructured Data Analysis,Networks,4,2,", subsets of vertices are organized into clusters with many edges connecting the vertices within the cluster and comparatively few clusters connecting to dif- ferent clusters"
Unstructured Data Analysis,Networks,4,2,A simple real-world example to keep in mind might be the face-to-face interaction (pre/post-pandemic
Unstructured Data Analysis,Networks,4,2,") of residents in New York City versus the face-to-face interaction of residents in London, both existing as clusters within a global network"
Unstructured Data Analysis,Networks,4,2,Another example might be tissues within certain organs within the human body
Unstructured Data Analysis,Networks,4,2,"These clusters, or communities, can be considered as relatively independent components of a graph"
Unstructured Data Analysis,Networks,4,2,"In a sense, the problem of community detection can be viewed as a graph clustering problem"
Unstructured Data Analysis,Networks,4,2,"Despite this analogy, there is no universally accepted quantitative definition of a community"
Unstructured Data Analysis,Networks,4,2,"Often, the concept of a community will be taken dependent on the specific system or application being studied"
Unstructured Data Analysis,Networks,4,2,From intuition and Example 5
Unstructured Data Analysis,Networks,4,2,"1, we get the notion that there must be more edges “inside” the community than edges linking vertices of the community with the rest of the graph"
Unstructured Data Analysis,Networks,4,2,"This is the reference guideline at the basis of most community definitions, but this strict requirement alone often isn’t enough to go about the problem of community detection in a disciplined manner and, more importantly, be able to interpret the final result"
Unstructured Data Analysis,Networks,4,2,"For example, in a very large network, can we really say that a community with 100,000 edges inside a community and 99,999 links to the rest of the graph is a satisfactory depiction of a community? In many cases, communities are algorithmically defined—they are just the final product of the algorithm, without a precise a priori definition"
Unstructured Data Analysis,Networks,4,2,Communities are parts of the graph with a few ties with the rest of the system
Unstructured Data Analysis,Networks,4,2,"To some extent, they can be considered as separate entities with their own autonomy"
Unstructured Data Analysis,Networks,4,2,"So, it makes sense to evaluate them independently of the graph as a whole"
Unstructured Data Analysis,Networks,4,2,"Local definitions focus on the sub- graph representing the community, including possibly its immediate neighborhood, but neglecting the rest of the graph"
Unstructured Data Analysis,Networks,4,2,Communities can also be defined with respect to the graph as a whole
Unstructured Data Analysis,Networks,4,2,This is reasonable in cases where certain clusters are essential parts of the graph and cannot be removed from the graph without seriously affecting the functioning of the system
Unstructured Data Analysis,Networks,4,2,"This idea has led to the proposal of indirect definitions in the literature; usually, some global property of the graph is used in an algorithm that delivers communities as its output"
Unstructured Data Analysis,Networks,4,2,"However, there is a class of proper definitions, based on the idea that a graph has community structure if it is not a random graph, or different from a random graph in some way"
Unstructured Data Analysis,Networks,4,2,"A random graph—such as an Erdo ̋s-–Rényi graph, which we saw in Example 3"
Unstructured Data Analysis,Networks,4,2,"5—is not expected to have community structure, because, by defini- tion, any two vertices have the same probability to be adjacent, so there is no preferential linking involving special groups of vertices"
Unstructured Data Analysis,Networks,4,2,"We can naturally assume that communities are groups of vertices sharing similar properties, so we can simply compute the similarity between each pair of vertices with respect to some reference property—local or global—no matter whether they are connected by an edge or not"
Unstructured Data Analysis,Networks,4,2,Each vertex gets assigned to the cluster whose vertices are most similar to it
Unstructured Data Analysis,Networks,4,2,"Similarity measures underlie traditional methods such as hierarchical, partitional, and spectral clustering"
Unstructured Data Analysis,Networks,4,2,"A partition is a division of a graph in clusters, such that each vertex belongs to one cluster"
Unstructured Data Analysis,Networks,4,2,A division of a graph into overlapping (or fuzzy) communities is called a cover
Unstructured Data Analysis,Networks,4,2,"Partitions can, however, be hierarchically ordered when the graph has different levels of or- ganization or structure at different scales"
Unstructured Data Analysis,Networks,4,2,"In this case, clusters display the community structure, with smaller communities inside, which may themselves contain still smaller communities"
Unstructured Data Analysis,Networks,4,2,"As an example, in a social network of children living in the same town, we can group the children according to the schools they attend, but within each school, we can further subdivide into year groups and classes"
Unstructured Data Analysis,Networks,4,2,"A natural way to represent the hierarchical structure of a graph is to draw a dendrogram, see Figure 5"
Unstructured Data Analysis,Networks,4,2,"Here, partitions of a graph with twelve vertices are shown"
Unstructured Data Analysis,Networks,4,2,"At the bottom, each vertex is its own module (the “leaves” of the tree)"
Unstructured Data Analysis,Networks,4,2,"By moving upwards, groups of vertices are successively aggregated"
Unstructured Data Analysis,Networks,4,2,Mergers of communities are represented by horizontal lines
Unstructured Data Analysis,Networks,4,2,The uppermost level represents the whole graph as a single community
Unstructured Data Analysis,Networks,4,2,"Cutting the diagram horizontally at some height, as shown in Figure 5 by the dashed line, displays one partition of the graph"
Unstructured Data Analysis,Networks,4,2,"In a community detection algorithm, it is almost impossible to determine the number of communities first, as we have just seen, so there there needs to be a way to measure whether each result is the relative best result during the process of the algorithm"
Unstructured Data Analysis,Networks,4,2,Modularity is used to measure whether the division of a community is “relatively good
Unstructured Data Analysis,Networks,4,2,” A relatively good result has higher similarity in nodes inside the community and lower similarity in nodes outside the community
Unstructured Data Analysis,Networks,4,2,"The Kernighan–Lin algorithm is one of the earliest methods proposed to detect communities and is still frequently used, often in combination with other techniques"
Unstructured Data Analysis,Networks,4,2,"The approach is motivated by the problem of partitioning electronic circuits onto boards, where the nodes contained in differ- ent boards need to be linked to each other with the least number of connections"
Unstructured Data Analysis,Networks,4,2,"The procedure optimizes some benefit function D, which represents the difference between the number of edges inside the modules and the number of edges between them"
Unstructured Data Analysis,Networks,4,2,The starting point is an initial partition of the graph in two clusters of predefined size; the initial partition can be random or suggested by some information on the graph structure
Unstructured Data Analysis,Networks,4,2,"Then subsets with equal numbers of vertices are swapped between the two groups, so that D has the maximal increase"
Unstructured Data Analysis,Networks,4,2,Notice here that this is a greedy algorithm
Unstructured Data Analysis,Networks,4,2,The subsets can consist of single vertices
Unstructured Data Analysis,Networks,4,2,"To reduce the risk of getting trapped in local maxima of D, the procedure includes some swaps that decrease the function D"
Unstructured Data Analysis,Networks,4,2,"After a series of swaps with positive and negative gains, the partition with the largest value of D is selected and used as starting point of a new series of iterations"
Unstructured Data Analysis,Networks,4,2,Divisive algorithms for community detection are a class of algorithms that rely on the observation that communities in a graph can be detected by identifying and removing connecting edges be- tween different communities
Unstructured Data Analysis,Networks,4,2,This disconnects communities from one another
Unstructured Data Analysis,Networks,4,2,The crucial task in divisive algorithms is to find a property of inter-community edges that we can use to identify them
Unstructured Data Analysis,Networks,4,2,"Divisive methods largely follow traditional techniques and conceptually, simply perform hi- erarchical clustering on the network"
Unstructured Data Analysis,Networks,4,2,"The main difference between divisive community detection and divisive hierarchical clustering is that in the latter, we remove inter-cluster edges, instead of edges between pairs of vertices with low similarity"
Unstructured Data Analysis,Networks,4,2,There is no a priori guarantee that inter-cluster edges connect vertices with low similarity
Unstructured Data Analysis,Networks,4,2,"In some cases, vertices with all their adjacent edges or whole subgraphs may be removed, instead of single edges"
Unstructured Data Analysis,Networks,4,2,An edge has high betweenness centrality if it lies on a large number of short paths between ver- tices
Unstructured Data Analysis,Networks,4,2,"Here, we focus on edges but betweenness centrality can also be defined for nodes"
Unstructured Data Analysis,Networks,4,2,"If we start at a node and want to go to some other node in the network, some edges will experience a lot more traffic than others, depending on the connectivity pattern of the network"
Unstructured Data Analysis,Networks,4,2,The betweenness centrality of an edge quantifies such traffic by considering strictly shortest paths (geodesic betweenness centrality) or densities of random walks (random walk betweenness centrality) between each pair of nodes and taking into account all possible pairs
Unstructured Data Analysis,Networks,4,2,"We can then identify communities through a process of ranking all of the edges based on their betweenness centrality, removing the edge with the largest value, and recalculating betweenness centrality for the remaining edges"
Unstructured Data Analysis,Networks,4,2,The recalculation step is important because the removal of an edge can cause a previously low-traffic edge to have much higher traffic
Unstructured Data Analysis,Networks,4,2,Implementing these steps iteratively gives a divisive algorithm for detecting community structure by deconstructing the initial graph into progressively smaller connected components until we finally obtain a set of isolated nodes
Unstructured Data Analysis,Networks,4,2,Classifying graphs is an important problem that plays a role in many applications
Unstructured Data Analysis,Networks,4,2,"For instance, if we have a network representing the skeleton of bone marrow vasculature in patients, we may wish to classify these networks as healthy, diseased, or treated patients"
Unstructured Data Analysis,Networks,4,2,"We see that the problem of graph classification is quite familiar with usual classification for
other more regularly structured datasets."
Unstructured Data Analysis,Networks,4,2,"For graphs in particular, the basic idea of classification is to compute several topological and label attributes for each graph in the dataset, and to use the derived feature-vector attributes for classification"
Unstructured Data Analysis,Networks,4,2,"(Here, the term topological means “related to the shape of the graph”—its configuration in terms of node placement and edge assignment"
Unstructured Data Analysis,Networks,4,2,") There are many graph attributes that we can look at, including average degree, average cluster- ing coefficient, average effective eccentricity, maximum effective eccentricity (effective diameter), minimum effective eccentricity (effective radius), average path length (closeness centrality), per- centage of central points, giant connected ratio, percentage of isolated points, percentage of end points, number of nodes, number of edges, spectral radius, second largest eigenvalue, trace, energy, number of eigenvalues, label entropy, neighborhood impurity, link impurity, and many more"
Unstructured Data Analysis,Networks,4,2,"The main challenge in classifying graphs, however, is their unstructured nature; specifically, the ques- tion is how to convert our discrete graph objects into numerical features or similarities for effective classification"
Unstructured Data Analysis,Networks,4,2,The shortest path kernel decomposes graphs into shortest paths and compares pairs of shortest paths according to their lengths and the labels of their endpoints
Unstructured Data Analysis,Networks,4,2,"Suggested by their name, the first task in constructing shortest path kernels is to transform the input graphs into shortest paths graphs"
Unstructured Data Analysis,Networks,4,2,"Given an input graph G = (V, E), we need to create a new graph S = (V, Es), which is its shortest path graph"
Unstructured Data Analysis,Networks,4,2,The shortest path graph S contains the same set of vertices as the graph from which it originates
Unstructured Data Analysis,Networks,4,2,"The edge set of the shortest path graph S, however, is a superset of the edge set of the graph from which it originates, since in the shortest path graph S, there exists an edge between all vertices which are connected by a walk in the original graph G"
Unstructured Data Analysis,Networks,4,2,"To complete the transformation, we assign labels to all the edges of the shortest path graph S"
Unstructured Data Analysis,Networks,4,2,The label of each edge is set equal to the shortest distance between its endpoints in the original graph G
Unstructured Data Analysis,Networks,4,2,Subtree kernels are based on common subtrees in the graphs
Unstructured Data Analysis,Networks,4,2,"The main idea is to consider pairs of nodes from Gi and Gj and see if they share common tree-like neighborhoods, i"
Unstructured Data Analysis,Networks,4,2,", we count the pairs of identical subtrees of height h rooted at vertex va ∈ Gi and vb ∈ Gj"
Unstructured Data Analysis,Networks,4,2,The kernel is defined as the sum over all pairs of vertices of a suitably defined vertex pair kernel
Unstructured Data Analysis,Networks,4,2,"The complexity of this approach is O(n2h4d), where d denotes the maximum degree"
Unstructured Data Analysis,Networks,4,2,The Weisfeiler–Lehman Kernel
Unstructured Data Analysis,Networks,4,2,The Weisfeiler–Lehman kernel is a fast subtree kernel that scales up for large labeled graphs
Unstructured Data Analysis,Networks,4,2,"It uses the Weisfeiler–Lehman isomorphism test, which uses iter- ative multiset-label determination, label compression, and relabeling steps and determines whether two graphs are isomorphic"
Unstructured Data Analysis,Networks,4,2,"Two graphs G, G′ are said to be isomorphic if there is a bijection f between their vertex sets such that any two vertices u and v in G are adjacent if and only if f(u) and f(v) are adjacent in G′"
Unstructured Data Analysis,Networks,4,2,Sorting the set of multisets allows for a straightforward definition and implementation of f for the compression of labels in Step 4: We keep a counter variable for f that records number of distinct strings that f has compressed before
Unstructured Data Analysis,Networks,4,2,"The role of f is to assign the current value of this counter to a string if an identical string has been compressed before, but when we encounter a new string, we increment the counter by 1 and f assigns its value to the new string"
Unstructured Data Analysis,Networks,4,2,"The sorted order of the set of multisets guarantees that all identical strings are mapped to the same number, because they occur in a consecutive block"
Unstructured Data Analysis,Networks,4,2,"However, note that the sorting of the set of multisets is not required to define f"
Unstructured Data Analysis,Networks,4,2,Any other injective mapping will give equivalent results
Unstructured Data Analysis,Networks,4,2,The alphabet Σ has to be sufficiently large for f to be injective
Unstructured Data Analysis,Networks,4,2,"For two graphs, |Σ| = 2n suffices"
Unstructured Data Analysis,Networks,4,2,"In each iteration i of the Weisfeiler–Lehman algorithm, we get a new labeling φi(v) for all nodes v"
Unstructured Data Analysis,Networks,4,2,"Recall that this labeling is concordant in G and G′, meaning that if nodes in G and G′ have identical multiset labels, and only in this case, they will get identical new labels"
Unstructured Data Analysis,Networks,4,2,"Therefore, we can view one iteration of Weisfeiler–Lehman relabeling as a function r((V, E, φi)) = (V, E, φi+1) that transforms all graphs in the same manner"
Unstructured Data Analysis,Networks,4,2,Notice that r depends on the set of graphs that we consider
Unstructured Data Analysis,Networks,4,2,First we import necessary modules and load the MUTAG dataset
Unstructured Data Analysis,Networks,4,2,"MUTAG is a collection of nitroaromatic compounds and here, the goal is to predict their mutagenicity on Salmonella ty- phimurium"
Unstructured Data Analysis,Text,4,2,"Input graphs are used to represent chemical compounds, where vertices represent atoms and are labeled by the atom type (represented by one-hot encoding), while edges between vertices represent bonds between the corresponding atoms"
Unstructured Data Analysis,Text,4,2,"Perhaps the oldest and most common way of storing and transferring information via writing, or text"
Unstructured Data Analysis,Text,4,2,"New technologies have made vast quantities of digital text available, providing crucial records of ever-increasing human interaction, communication, and culture"
Unstructured Data Analysis,Text,4,2,"For social scientists, the information encoded in text is a rich complement to the more structured kinds of data traditionally used in research; recent years have seen an explosion of empirical economics research using text as data"
Unstructured Data Analysis,Text,4,2,"Here are a few recent and very relevant examples of the economic and financial importance and impact of text data: In finance, text from financial news, social media, and company filings is used to predict asset price movements and study the causal impact of new information"
Unstructured Data Analysis,Text,4,2,"In macroeconomics, text is used to forecast variation in inflation and unemployment, and estimate the effects of policy uncertainty"
Unstructured Data Analysis,Text,4,2,"In media economics, text from news and social media is used to study the drivers and effects of political slant"
Unstructured Data Analysis,Text,4,2,"In industrial organization and marketing, text from advertisements and product reviews is used to study the drivers of consumer decision making"
Unstructured Data Analysis,Text,4,2,"In political economy, text from politicians’ speeches is used to study the dynamics of political agendas and debate"
Unstructured Data Analysis,Text,4,2,Text is inherently high dimensional
Unstructured Data Analysis,Text,4,2,"Suppose that we have a sample of documents, each w words long, and suppose that each word is drawn from a vocabulary of p possible words"
Unstructured Data Analysis,Text,4,2,Then the unique representation of these documents has dimension pw
Unstructured Data Analysis,Text,4,2,"A sample of 30-word Tweets that use only the one thousand most common words in the English language, for example, has roughly as many dimensions as there are atoms in the universe"
Unstructured Data Analysis,Text,4,2,"A consequence is that the statistical methods used to analyze text are closely related to those used to analyze high-dimensional data in other domains, such as machine learning and computational biology"
Unstructured Data Analysis,Text,4,2,"When we read text, we do not see a vector of dummy variables, nor a sequence of unrelated tokens"
Unstructured Data Analysis,Text,4,2,"They interpret words in light of other words, and we extract meaning from the text as a whole"
Unstructured Data Analysis,Text,4,2,The field of computational linguistics has made tremendous progress in this kind of interpretation
Unstructured Data Analysis,Text,4,2,Most of us have mobile phones that are capable of complex speech recognition
Unstructured Data Analysis,Text,4,2,"Algorithms exist to efficiently parse grammatical structure, disambiguate different senses of words (this task is known as word sense disambiguation in machine learning), distinguish key points from secondary asides, and so on"
Unstructured Data Analysis,Text,4,2,"Yet virtually all analysis of text in the social sciences, like much of the text analysis in machine learning more generally, does not take into account these complex aspects"
Unstructured Data Analysis,Text,4,2,"Raw text consists of an ordered sequence of language elements: words, punctuation, and white space"
Unstructured Data Analysis,Text,4,2,"To reduce this to a simpler representation suitable for statistical analysis, we typically make three kinds of simplifications: dividing the text into individual documents i; reducing the number of language elements we consider; and limiting the extent to which we encode dependence among elements within documents"
Unstructured Data Analysis,Text,4,2,The result is a mapping from raw text D to a numerical array C
Unstructured Data Analysis,Text,4,2,A row ci of C is a numerical vector with each element indicating the presence or count of a particular language token in document i
Unstructured Data Analysis,Text,4,2,The first step in constructing C is to divide raw text D into individual documents {Di}
Unstructured Data Analysis,Text,4,2,"In many applications, this is governed by the level at which the attributes of interest V are defined"
Unstructured Data Analysis,Text,4,2,"For spam detection, the outcome of interest is defined at the level of individual emails, so we want to divide text that way too"
Unstructured Data Analysis,Text,4,2,"If V is daily stock price movements that we wish to predict from the prior day’s news text, it might make sense to divide the news text by day as well"
Unstructured Data Analysis,Text,4,2,"In other cases, the natural way to define a document is not so clear"
Unstructured Data Analysis,Text,4,2,"If we wish to predict legislators’ partisanship from their floor speeches, we could aggregate speech so a document is a speaker–day, a speaker-year, or all speech by a given speaker during the time they are in congress"
Unstructured Data Analysis,Text,4,2,"When we use methods that treat documents as independent (which is true most of the time), finer partitions will typically ease computation at the cost of limiting the dependence we are able to capture"
Unstructured Data Analysis,Text,4,2,"Theoretical guidance for the right level of aggregation is often limited, so this is an important dimension along which to check the sensitivity of results"
Unstructured Data Analysis,Text,4,2,"To reduce the number of features to a manageable load, a common first step is to strip out ele- ments of the raw text other than words"
Unstructured Data Analysis,Text,4,2,"This might include punctuation, numbers, HTML tags, proper names, and so on"
Unstructured Data Analysis,Text,4,2,It is also common to remove a subset of words that are either very com- mon or very rare
Unstructured Data Analysis,Text,4,2,"Very common words, often called “stop words,” include articles (“the,” “a”), conjunctions (“and,” “or”), forms of the verb “to be,” and so on"
Unstructured Data Analysis,Text,4,2,"These words are important to the grammatical structure of sentences, but they typically convey relatively little meaning on their own"
Unstructured Data Analysis,Text,4,2,"The frequency of “the” is probably not very diagnostic of whether an email is spam, for example"
Unstructured Data Analysis,Text,4,2,Common practice is to exclude stop words based on a predefined list
Unstructured Data Analysis,Text,4,2,An approach that excludes both common and rare words and has proved very useful in practice is filtering by “term frequency inverse document frequency” (TF-IDF).
Unstructured Data Analysis,Text,4,2,Very common words that appear in most or all documents will also have low TF-IDF scores because idfj will be low
Unstructured Data Analysis,Text,4,2,Note that this improves on simply excluding words that occur frequently because it will keep words that occur frequently in some documents but do not appear in others; these often provide useful information
Unstructured Data Analysis,Text,4,2,A common practice is to keep only the words within each document i with TF-IDF scores above some rank or cutoff
Unstructured Data Analysis,Text,4,2,"A final step that is commonly used to reduce the feature space is stemming: replacing words with their root such that, e"
Unstructured Data Analysis,Text,4,2,", “economic,” “economics,” “eco- nomically” are all replaced by the stem “economic"
Unstructured Data Analysis,Text,4,2,” The Porter stemmer is a standard stemming tool for English language text
Unstructured Data Analysis,Text,4,2,All of these cleaning steps reduce the number of unique language elements we must consider and thus the dimensionality of the data
Unstructured Data Analysis,Text,4,2,This can provide a massive computational benefit and it is also often key to getting more interpretable model fits (e
Unstructured Data Analysis,Text,4,2,", in topic modeling)"
Unstructured Data Analysis,Text,4,2,"However, each of these steps requires careful decisions about the elements likely to carry meaning in a particular application"
Unstructured Data Analysis,Text,4,2,One researcher’s stop words may be another’s subject of interest
Unstructured Data Analysis,Text,4,2,Dropping numerals from political text means missing references to “the first 100 days” or “September 11
Unstructured Data Analysis,Text,4,2,"” In online communication, even punctuation can no longer be stripped without potentially significant information loss"
Unstructured Data Analysis,Text,4,2,Producing a tractable representation also requires that we limit dependence among language ele- ments
Unstructured Data Analysis,Text,4,2,"A fairly mild step in this direction, for example, might be to parse documents into distinct sentences and encode features of these sentences while ignoring the order in which they occur"
Unstructured Data Analysis,Text,4,2,The most common methodologies go much further The simplest and most common way to represent a document is as a bag-of-words
Unstructured Data Analysis,Text,4,2,"The order of words is ignored altogether, and ci is a vector whose length is equal to the number of words in the vocabulary and whose elements cij are the number of times word j occurs in document i"
Unstructured Data Analysis,Text,4,2,This scheme can be extended to encode a limited amount of dependence by counting unique phrases rather than unique words
Unstructured Data Analysis,Text,4,2,A phrase of length n is referred to as an n-gram
Unstructured Data Analysis,Text,4,2,"For example, in our snippet above, the count of 2-grams (or bigrams) would have cij = 2 for j = good"
Unstructured Data Analysis,Text,4,2,"night, cij = 1 for j including night"
Unstructured Data Analysis,Text,4,2,"good, night"
Unstructured Data Analysis,Text,4,2,"part, part"
Unstructured Data Analysis,Text,4,2,"sweet, and sweet"
Unstructured Data Analysis,Text,4,2,"sorrow, and cij = 0 for all other possible 2-grams"
Unstructured Data Analysis,Text,4,2,The bag-of-words representation then corresponds to counts of 1-grams
Unstructured Data Analysis,Text,4,2,Counting n-grams of order n > 1 yields data that describe a limited amount of the dependence between words
Unstructured Data Analysis,Text,4,2,"Specifically, the n-gram counts are sufficient for estimation of an n-order homo- geneous Markov model across words (i"
Unstructured Data Analysis,Text,4,2,", the model that arises if we assume that word choice is only dependent upon the previous n words)"
Unstructured Data Analysis,Text,4,2,This can lead to richer modeling
Unstructured Data Analysis,Text,4,2,"In analysis of partisan speech, for example, single words are often insufficient to capture the patterns of interest: “death tax” and “tax break” are phrases with strong partisan overtones that are not evident if we look at the single words “death,” “tax,” and “break"
Unstructured Data Analysis,Text,4,2,"Unfortunately, the dimension of ci increases exponentially quickly with the order n of the phrases tracked"
Unstructured Data Analysis,Text,4,2,The majority of text analyses consider n-grams up to 2 or 3 at most and the ubiquity of these simple representations (in both machine learning and social science) reflects a belief that the return to richer n-gram modeling is usually small relative to the cost
Unstructured Data Analysis,Text,4,2,The best practice in many cases is to begin analysis by focusing on single words
Unstructured Data Analysis,Text,4,2,"Given the accuracy obtained with words alone, we can then evaluate if it is worth the extra time to move on to 2-grams or 3-grams"
Unstructured Data Analysis,Text,4,2,"While rarely used in the social sciences literature to date, there is a vast array of methods from computational linguistics that capture richer features of text and may have high return in certain applications"
Unstructured Data Analysis,Text,4,2,One basic step beyond the simple n-gram counting above is to use sentence syntax to inform the text tokens used to summarize a document
Unstructured Data Analysis,Text,4,2,"An alternative approach is to move beyond treating documents as counts of language tokens, and to instead consider the ordered sequence of transitions between words"
Unstructured Data Analysis,Text,4,2,"In this case, one would typically break the document into sentences and treat each as a separate unit for analysis"
Unstructured Data Analysis,Text,4,2,A single sentence of length s (i
Unstructured Data Analysis,Text,4,2,", containing s words) is then represented as a binary p × s matrix S, where the nonzero elements of S indicate occurrence of the row-word in the column-position within the sentence, and p is the length of the vocabulary"
Unstructured Data Analysis,Text,4,2,"Such representations lead to a massive increase in the dimensions of the data to be modeled and analysis of this data tends to proceed through word embedding: the mapping of words to a location in Rm for some m ≪ p, such that the sentences are then sequences of points in this m dimensional space"
Unstructured Data Analysis,Text,4,2,"Similar to what we have seen with networks, a common way to bypass the unstructured nature of text data and the lack of a vector representation for the statistical and machine learning tasks we might want to do with text data is to embed the text into some vector space"
Unstructured Data Analysis,Text,4,2,"In general, this strategy of embedding or vectorization can be adapted to any unstructured data type we may wish to work with"
Unstructured Data Analysis,Text,4,2,"The challenge—as we have seen with networks, and as we will see with text data—is making sure that the vectorization preserves the “right” features of the original unstructured data"
Unstructured Data Analysis,Text,4,2,Most of the existing and fundamental machine and deep learning architectures that are widely popular and powerful cannot intake nor directly use natural languages such as English (or any other language) because they do not understand word or phrases
Unstructured Data Analysis,Text,4,2,"Their input is numerical data, which requires the text information from languages to be represented as numerical values or as word embeddings"
Unstructured Data Analysis,Text,4,2,"Word embeddings are essentially the numerical representation of the semantic meaning of a document text, in the form of vectors"
Unstructured Data Analysis,Text,4,2,"These vectors provide the relation between various words or phrases of the document, i"
Unstructured Data Analysis,Text,4,2,", the words having similar meaning have closer vector values, which establish their closeness in the linguistics"
Unstructured Data Analysis,Text,4,2,"For example, the word “male” is proximate to “king” and “boy” and quite far from “queen” and “princess"
Unstructured Data Analysis,Text,4,2,"” Note that word embedding is not just the representation of text into numbers or vectors, but conveys the semiotic meanings of the words"
Unstructured Data Analysis,Text,4,2,The distance between vectors represents the similarity between the words
Unstructured Data Analysis,Text,4,2,Word embeddings can be categorized broadly into two categories: frequency-based and prediction- based
Unstructured Data Analysis,Text,4,2,"As the name suggests, frequency-based embeddings take into account the frequency or number of times a word occurs in the document to find its relevance"
Unstructured Data Analysis,Text,4,2,"It is also called count-based embeddings, and its principal point is to give weights according to the occurrence and also the con- text"
Unstructured Data Analysis,Text,4,2,"Frequency-based word embeddings can again be classified into count vector, TF-IDF vector, and co-occurrence"
Unstructured Data Analysis,Text,4,2,Prediction-based embeddings fundamentally consist of two main approaches: continuous bag-of-words (CBOW) and skip-gram that come together to form the Word2Vec which
Unstructured Data Analysis,Text,4,2,Count vectorizing or one-hot encoding (OHE) is the most basic word em- bedding that works on a simple binary principle and produces a high-dimensional sparse matrix
Unstructured Data Analysis,Text,4,2,"Firstly, it creates a bag-of-words (vocabulary list) from all the given text or corpus, which is defined as the assemblage of similar text"
Unstructured Data Analysis,Text,4,2,"Secondly, it counts the occurrence of each word in the document existing in the corpus"
Unstructured Data Analysis,Text,4,2,"The final output of count vectorizing is a sparse matrix with dimensions D × T , where D denotes the number of documents and T signifies the number of dissimilar words in the vocabulary"
Unstructured Data Analysis,Text,4,2,This can be understood by the following example:
Unstructured Data Analysis,Text,4,2,"As displayed above, there are two documents, i"
Unstructured Data Analysis,Text,4,2,", D = 2 and total eight different words, thus making T = 8 in the corpus: {“the”, “dog”, “ate”, “cat”, “lion”, “can”, “eat”, “a”}"
Unstructured Data Analysis,Text,4,2,"Hence, count vectorizing encoding for the above corpus should be as shown in Table 1"
Unstructured Data Analysis,Text,4,2,"The matrix generated and represented in Table 1 has the following interpretation: If a word appears in a corresponding document, it is given the value “1,” othrerwise it is given the value “0"
Unstructured Data Analysis,Text,4,2,"” If a word appears multiple times, it can be given that frequency, just like “the” occurs twice in D1, so the value of “the” is marked as 2 for D1"
Unstructured Data Analysis,Text,4,2,"As the words “lion,” “can,” “eat,” and “a” are absent in D1, hence they are assigned “0” for D1"
Unstructured Data Analysis,Text,4,2,"Similarly, the words “dog” and “ate” are not given in D2 and marked “0"
Unstructured Data Analysis,Text,4,2,” Each cell in the matrix corresponds to one document and one precise word in the corpus
Unstructured Data Analysis,Text,4,2,"Usually, a corpus contains thousands of sentences with thousands of words, which means that many words may not occur in most of the documents"
Unstructured Data Analysis,Text,4,2,"Thus, a significant drawback in this em- bedding is the occurrence of cells with may 0-values resulting in a sparsity matrix with high di- mensions"
Unstructured Data Analysis,Text,4,2,"Nonetheless, these dimensions or features can be diminished so that visualization is possible in lower dimensions"
Unstructured Data Analysis,Text,4,2,One way of doing this is to eliminate commonly occurring words (also called “stopwords” previously discussed) from the corpus
Unstructured Data Analysis,Text,4,2,"This whole process of generating table is called tokenization, i"
Unstructured Data Analysis,Text,4,2,", identifying all the individual words present in the corpus"
Unstructured Data Analysis,Text,4,2,Count vectorization is also known as Bag-of-Words (BOW) vectorization
Unstructured Data Analysis,Text,4,2,"Here, we choose the term “count vectorization” to avoid confusion with the continuous bag-of-words model that will be presented further on"
Unstructured Data Analysis,Text,4,2,"As we’ve seen above, the full abbreviation of TF-IDF is “term frequency- inverse document frequency"
Unstructured Data Analysis,Text,4,2,"” The phrase “term frequency” corresponds to the incidence of a term in a document divided by the entire amount of documents, while the phrase “inverse document frequency” corresponds to the base 10 logarithmic value of a whole number of documents divided by the magnitude of documents"
Unstructured Data Analysis,Text,4,2,"The product of TF and IDF values for a word is known as its TF-IDF weight; the higher the TF-IDF weight, the more rare a term, and vice versa"
Unstructured Data Analysis,Text,4,2,"TF-IDF vectorization bypasses the problem we’ve seen in count vectorizing, where the occurrence of a particular word in a document is simply recorded, regardless of its significance"
Unstructured Data Analysis,Text,4,2,"Commonly used words—such as articles (“a,” “an,” “the”) and forms of the verb “to be” (“is,” “am,” “are”)—are most likely to appear in the documents, and their frequency is not indicative of their importance; on the contrary, they tend to be the least important"
Unstructured Data Analysis,Text,4,2,TF-IDF tries to eliminate these frequent words by assigning them lower weights and strives to include more noteworthy terms by bolstering them with higher weight values
Unstructured Data Analysis,Text,4,2,TF-IDF values can be computed on every word of the corpus and terms with higher TF-IDF values will give some measure of prominence
Unstructured Data Analysis,Text,4,2,The terms with higher TF-IDF values can be aligned with their search volumes on the web and selecting the terms with greater search volumes as they make more sense according to the users
Unstructured Data Analysis,Text,4,2,A drawback of TF-IDF vectorization is that it also produces a high-dimensional representation that does not necessarily capture the actual semantic relationship between words
Unstructured Data Analysis,Text,4,2,Continuous Bag-of-Words (CBOW) Model
Unstructured Data Analysis,Text,4,2,"Bag-of-words (BOW), as discussed above in count vectorization, basically involves two components: a dictionary of recognized words and a way to quantify the presence of these words"
Unstructured Data Analysis,Text,4,2,The continuous bag-of-words (CBOW) model is one of the fundamental approaches based on BOW and is used for word embeddings
Unstructured Data Analysis,Text,4,2,"It is generally rec- ognized as a learning model that predicts a term by its context, which may be a single word or multiple words"
Unstructured Data Analysis,Text,4,2,"In CBOW, for every word, c words before and after are considered to check the semantic association among words"
Unstructured Data Analysis,Text,4,2,It is based on neural networks with hidden layer(s) and works on a simple assumption that the milieu of a word or phrase can be branded by the neighboring words
Unstructured Data Analysis,Text,4,2,"The dimension of the hidden layer and the output layer should remain the same, but the input layer dimensions can be altered along with the activation function of the hidden layer"
Unstructured Data Analysis,Text,4,2,"To embed input context words, we use the one-hot representation for each word initially, and apply V ⊤ to get the corresponding word vector embeddings of dimension N"
Unstructured Data Analysis,Text,4,2,We apply U ⊤ to an input word vector to generate a score vector and use the softmax operation to convert a score vector into a probability vector of size W (recall that we’ve already briefly encountered the notion of softmax functions in the first topic on images)
Unstructured Data Analysis,Text,4,2,This process yields a probability vector that matches the vector representation of the output word
Unstructured Data Analysis,Text,4,2,Initial values for matrices V and U are randomly assigned
Unstructured Data Analysis,Text,4,2,The dimension N of word embed- ding can vary based on different application scenarios
Unstructured Data Analysis,Text,4,2,"Usually, it ranges from 50 to 300 dimen- sions"
Unstructured Data Analysis,Text,4,2,"After obtaining both matrices V or U , they can either be used solely or averaged to obtained"
Unstructured Data Analysis,Text,4,2,The skip-gram model (or continuous skip-gram model) is an alternative to and variant of the CBOW model
Unstructured Data Analysis,Text,4,2,"Instead of using the surrounding words to anchor the middle word and derive its context, it learns an embedding by predicting the surrounding words with the help of a given central word"
Unstructured Data Analysis,Text,4,2,"In other words, the roles of the central word and window words are reversed"
Unstructured Data Analysis,Text,4,2,"In this sense, the skip-gram architecture can be seen as the complement of CBOW that maintains the topology of the words"
Unstructured Data Analysis,Text,4,2,The skip-gram model is analogous to the one-context architecture of CBOW
Unstructured Data Analysis,Text,4,2,"The skip-gram approach predicts the neighboring words using the current word, i"
Unstructured Data Analysis,Text,4,2,", each cen- tral word is given as the input to a classifier with constant projection layer and predicts the nearby words within a particular range previous to and next to the current word"
Unstructured Data Analysis,Text,4,2,The training purpose is to learn a word vector by forecasting the close by words
Unstructured Data Analysis,Text,4,2,The most important feature of skip-gram models is that it captures multiple semantics for each word
Unstructured Data Analysis,Text,4,2,"Another key advantage of skip-gram models is their association with negative sampling, which outperforms other competitors"
Unstructured Data Analysis,Text,4,2,Some examples are the negative sampling and hierarchical softmax: Negative sampling is a method that maximizes the log probability of the softmax model by only summing over a smaller subset of W words.
Unstructured Data Analysis,Text,4,2,"How exactly does negative sampling in the skip-gram model work? Instead of just using pos- itive samples, we add some negative samples too"
Unstructured Data Analysis,Text,4,2,"Since the ultimate aim of every embedding algorithm is to better predict the context words according to a particular word, each model takes input and output words and its output score forecasts whether these words are neighbors or not in the embedding (vector) space"
Unstructured Data Analysis,Text,4,2,"If the words are neighbors, the target is set to “1,” otherwise, it is given the value “0"
Unstructured Data Analysis,Text,4,2,” A desirable model may generate 100% correctness by assigning all 1s but it will be unable to learn and generalize
Unstructured Data Analysis,Text,4,2,"To overcome this, negative samples must be added to the dataset to obtain 0s (randomly designated words that are not neighbors as the output terms)"
Unstructured Data Analysis,Text,4,2,"Here’s an example to illustrate the idea: If a bag contains 6 red balls, 12 white balls and 2 black balls, then simple sampling has a probability of picking red balls is 6/20=0.3, the black ball is 0.1, and the white ball is 0.6"
Unstructured Data Analysis,Text,4,2,"The main shortcoming here is that high-frequency data will be picked (because of their higher probability), which typically has a smaller value"
Unstructured Data Analysis,Text,4,2,"So we can do adjusted sampling instead, i"
Unstructured Data Analysis,Text,4,2,", reduce the probability by 3/4, which reduces the white ball probability to 0.45 , which, as a result, means that the probabilities of red balls and black balls will be enhanced"
Unstructured Data Analysis,Text,4,2,"The appropriate number of negative samples varies between 5 and 20, although 2 to 5 negative samples are also sufficient"
Unstructured Data Analysis,Text,4,2,Word2Vec
Unstructured Data Analysis,Text,4,2,Word2Vec is a vast simplification of the word embedding process; it is based on the amalgamation of the shallow neural network architectures CBOW and skip-gram models and maps words to their target terms
Unstructured Data Analysis,Text,4,2,"It can also be seen as a successor to the neural probabilistic model, which embeds words by classification"
Unstructured Data Analysis,Text,4,2,"Word2Vec was published in a research paper by Google employees Tomas Mikolov, Kai Chen, Greg Corrado and Jeffrey Dean in September 2013 [7]; in this paper, they used Word2Vec to train on 100 billion words taken from Google News Data with nearly 300 dimensions"
Unstructured Data Analysis,Text,4,2,Word2Vec is largely considered to be the word embedding standard; it produces generalized results in lower dimensions
Unstructured Data Analysis,Text,4,2,It is based on a working principle of using simple two-layer neural networks for the training of rebuilding linguistic backgrounds of the word
Unstructured Data Analysis,Text,4,2,"The input of Word2Vec is usually an enormous text corpus and produces “vector space” of very large dimensions, which amounts to assigning a corresponding vector in the space to each exclusive word"
Unstructured Data Analysis,Text,4,2,"In this assignment, words of a corpus that share mutual context are located; they are adjacent to each other in the vector space and are then called word vectors"
Unstructured Data Analysis,Text,4,2,The metric used here is the cosine similarity or Euclidean distance which describes the likeness between words or documents
Unstructured Data Analysis,Text,4,2,"Cosine similarity can be defined as the value of the cosine of the angle between two nonzero vectors in three-dimensional space: the smaller the cosine angle, closer the documents"
Unstructured Data Analysis,Text,4,2,"Since Word2Vec is based on both CBOW and skip-gram, which are two similar yet opposite architectures and have their respective pros and cons, these show up and in some sense, cancel each other out, in Word2Vec"
Unstructured Data Analysis,Text,4,2,"On the one hand, CBOW is quicker and produces enhanced word presentations for more recurrent terms"
Unstructured Data Analysis,Text,4,2,"On the other hand, skip-gram is better suited for rare words and also works well with a small vocabulary"
Unstructured Data Analysis,Text,4,2,These pre-trained word vectors can be downloaded online and it comes in different shapes and sizes
Unstructured Data Analysis,Text,4,2,"The smallest one contains 6 billion tokens with 50, 100, 200, or 300 dimensions, which is 822 MB in size"
Unstructured Data Analysis,Text,4,2,The next smallest file contains 42 billion tokens with 300 dimensions of 1
Unstructured Data Analysis,Text,4,2,75 GB size
Unstructured Data Analysis,Text,4,2,The largest file is trained on 840 billion tokens with 300 dimensions and of size 2
Unstructured Data Analysis,Text,4,2,03 GB
Unstructured Data Analysis,Text,4,2,"There is additionally a pre-trained file for Twitter data that contains 2 billion tokens of 25, 50, 100, and 200 dimensions of 1"
Unstructured Data Analysis,Text,4,2,42 GB in size
Unstructured Data Analysis,Text,4,2,"Different local contexts around a word should be indicative of specific properties of the word, e"
Unstructured Data Analysis,Text,4,2,", the plural or singular form, the tenses, etc"
Unstructured Data Analysis,Text,4,2,Embedding models should be able to discern differences in the contexts and encode these details into a mean- ingful representation in the word vector space
Unstructured Data Analysis,Text,4,2,All senses (or meanings) of a word should be rep- resented
Unstructured Data Analysis,Text,4,2,Models should be able to discern the sense of a word from its context and find the appropriate embedding
Unstructured Data Analysis,Text,4,2,This is needed to achieve embeddings to distinguish between a bank as a financial institution and a bank as the terrain alongside a river
Unstructured Data Analysis,Text,4,2,Results of a word embedding model should be reliable
Unstructured Data Analysis,Text,4,2,This is important since word vectors are randomly initialized when being trained
Unstructured Data Analysis,Text,4,2,"Even if a model creates different representations from the same dataset because of random initialization, the performance of various representations should score consistently"
Unstructured Data Analysis,Text,4,2,The geometry of an embedding space should have a good spread
Unstructured Data Analysis,Text,4,2,"Generally speaking, a smaller set of more frequent, unrelated words should be evenly distributed throughout the space while a larger set of rare words should cluster around frequent words"
Unstructured Data Analysis,Text,4,2,Word models should overcome the difficulty arising from inconsistent frequency of word usage and derive some meaning from word frequency
Unstructured Data Analysis,Text,4,2,The word similarity evaluator captures the relationship between word vectors and human perceived semantic similarity in terms of a distance.
Unstructured Data Analysis,Text,4,2,"On the other hand, it has several problems; see [4] for further detail"
Unstructured Data Analysis,Text,4,2,"This test is aimed at finding the distributional similarity among pairs of words, but similarity may be confused with relatedness"
Unstructured Data Analysis,Text,4,2,"For example, “car” and “train” are two similar words while “car” and “road” are two related words"
Unstructured Data Analysis,Text,4,2,"In addition to other limitations, this limitation restricts the effectiveness and comprehensiveness of this evaluator"
Unstructured Data Analysis,Text,4,2,"Thus, high cosine similarity means that vectors share a similar direction"
Unstructured Data Analysis,Text,4,2,"However, it is impor- tant to note that the 3CosAdd method normalizes vector lengths using the cosine similarity [8]"
Unstructured Data Analysis,Text,4,2,Concept categorization uses an approach different from both word similarity and word analogy
Unstructured Data Analysis,Text,4,2,"Here, the goal is to split a given set of words into different categorical subsets of words"
Unstructured Data Analysis,Text,4,2,"For example, given the task of separating words into two categories, the model should be able to categorize words “sandwich,” “tea,” “pasta,” and “water” into two groups"
Unstructured Data Analysis,Text,4,2,"To do this, first, the corresponding vector to each word is calculated"
Unstructured Data Analysis,Text,4,2,"Then, a clustering al- gorithm (e"
Unstructured Data Analysis,Text,4,2,", k-means clustering) is used to separate the set of word vectors into n different categories"
Unstructured Data Analysis,Text,4,2,"A performance metric is then defined based on cluster’s purity, where purity refers to whether each cluster contains concepts from the same or different categories"
Unstructured Data Analysis,Text,4,2,"The challenges associated with approach are that first, the datasets do not have standardized splits"
Unstructured Data Analysis,Text,4,2,"Second, no specific clustering methods are defined for this evaluator"
Unstructured Data Analysis,Text,4,2,"It is important to note that clustering can be computationally expensive, especially when there are many words as well as categories"
Unstructured Data Analysis,Text,4,2,"Third, the clustering methods may be unreliable if there are either uneven distributions of word vectors or no clearly defined clusters"
Unstructured Data Analysis,Text,4,2,Subjectivity is one of the main issues associated with this evaluator
Unstructured Data Analysis,Text,4,2,"As humans, we are able to group words by inference using concepts that word embeddings can gloss over"
Unstructured Data Analysis,Text,4,2,"Given the words “lemon,” “sun,” “banana,” “blueberry,” “ocean,” “iris"
Unstructured Data Analysis,Text,4,2,"” One possible grouping could be as yel- low objects (“lemon,” “sun,” “banana”) and red objects (“blueberry,” “ocean,” “iris”) [10]"
Unstructured Data Analysis,Text,4,2,"Since words can belong to multiple categories, another grouping could be given by “lemon,” “banana,” “blueberry,” and “iris” being in the plant category while “sun” and “ocean” are in the nature cate- gory"
Unstructured Data Analysis,Text,4,2,"However, due to the uncompromising nature of the performance metric, there is no adequate method in evaluating each cluster’s quality"
Unstructured Data Analysis,Text,4,2,"The property that the sets of words and categories aim to test for is semantic relation, since the words are grouped into concept categories"
Unstructured Data Analysis,Text,4,2,One advantageous property of this evaluator is its ability to test for the frequency effect and the hub-ness problem since it is good at revealing whether frequent words are clustered together
Unstructured Data Analysis,Text,4,2,"Comparatively, this evaluator has been less researched than word similarity and word analogy, yet, it provides a good measure and way to assess the geometry of an embedding space"
Unstructured Data Analysis,Text,4,2,"If frequent words are clustered to form hubs while rarer words are not clustered around the more frequent words they relate to, the evaluator will not perform well using this metric"
Unstructured Data Analysis,Text,4,2,There is subjectivity involved in this evaluator as the relationship of different word groups can be interpreted in different ways
Unstructured Data Analysis,Text,4,2,"Also, being similar to the word analogy evaluator, this evaluator relies heavily on human reasoning and logic"
Unstructured Data Analysis,Text,4,2,The outliers identified by humans are strongly influ- enced by the characteristics of words perceived to be important
Unstructured Data Analysis,Text,4,2,"Yet, the recognized patterns might not be immediately clear to word embedding models"
Unstructured Data Analysis,Text,4,2,"Good testing data: To ensure a reliable representative score, testing data should be varied with a good spread in the span of a word space"
Unstructured Data Analysis,Text,4,2,Frequently and rarely occurring words should be included in the evaluation
Unstructured Data Analysis,Text,4,2,Data should be reliable in the sense that they are correct and objective
Unstructured Data Analysis,Text,4,2,"Ideally, an evaluator should test for many properties of a word embedding model"
Unstructured Data Analysis,Text,4,2,This is not only an important property for giving a representative score but also for determining the effectiveness of an evaluator
Unstructured Data Analysis,Text,4,2,The score of a word model in an intrinsic evaluation task should correlate well with the performance of the model in downstream natural language processing tasks
Unstructured Data Analysis,Text,4,2,This is important for determining the effectiveness of an evaluator
Unstructured Data Analysis,Text,4,2,Evaluators should be computationally efficient
Unstructured Data Analysis,Text,4,2,Most models are created to solve computationally expensive downstream tasks
Unstructured Data Analysis,Text,4,2,Model evaluators should be simple yet able to predict the downstream performance of a model
Unstructured Data Analysis,Text,4,2,"The performance of different word embedding models with respect to an evaluator should have enough statistical significance, or enough variance between score distributions, to be differentiated"
Unstructured Data Analysis,Text,4,2,This is needed in judging whether a model is better than another and helpful in determining performance rankings between models
Unstructured Data Analysis,Text,4,2,"A language is a set of sentences of finite length, constructed using a finite alphabet set, or in terms of language syntax, they are constructed using a finite vocabulary of symbols"
Unstructured Data Analysis,Text,4,2,"Since the alphabet set is finite, as well as the length of the sentences, the set of sentences (in a language) is also finite"
Unstructured Data Analysis,Text,4,2,"For example, if alphabet set is of size two, and length of sentences is ten, there can be only 1024 maximum number of possible sentences"
Unstructured Data Analysis,Text,4,2,"When we are interested in an infinite language set, say L, we can use the finite devices called generating grammars to investigate the structure of L"
Unstructured Data Analysis,Text,4,2,"In that scenario, the theory of language will contain specification of a class of functions from which grammars for a particular language may be drawn"
Unstructured Data Analysis,Text,4,2,"Natural language processing (NLP) is a collection of computational techniques for the automatic analysis and representation of human languages, motivated by theory"
Unstructured Data Analysis,Text,4,2,"However, the automatic analysis of text, at par with humans, requires a far deeper understanding of natural language by machines, which is still far from a practical reality"
Unstructured Data Analysis,Text,4,2,"There are many examples of NLP—such as online information retrieval (IR), aggregation, and question-answering—that have been mainly based on algorithms relying on the textual representation of web pages, as well NLP to some extent"
Unstructured Data Analysis,Text,4,2,"Such algorithms are very good at retrieving texts (IR), splitting it into parts, checking the spellings, and and word-level analysis, but unfortunately not as successful for analysis at sentence and paragraph level"
Unstructured Data Analysis,Text,4,2,All the above capabilities are needed to shift from simple NLP to natural language understanding
Unstructured Data Analysis,Text,4,2,"The current approaches to NLP are based on the syntactic representation (also called syntactic structures) of text, i.e"
Unstructured Data Analysis,Text,4,2,", they rely on the word co-occurrence frequencies"
Unstructured Data Analysis,Text,4,2,"Such algorithms have the limitation that they can process only based on the information they can see in the text being processed, but cannot consider the background information such as humans do"
Unstructured Data Analysis,Text,4,2,"For example, when we say that “Sachin Tendulkar is a good batsman,” we understand this sentence due to abundant information we have in our brains about the game of cricket and about the successes of “Sachin Tendulkar” in many cricket matches"
Unstructured Data Analysis,Text,4,2,"However, the currently-used NLP algorithms do not have this kind of background, so their understanding is limited about any given text"
Unstructured Data Analysis,Text,4,2,NLP and its extension to natural language understanding is currently a very active and popular research topic in machine learning
Unstructured Data Analysis,Text,4,2,Many new computational models are being proposed and at- tempt to bridge the cognitive gap by emulating the processes recognized as being part of the human brain that is used for language processing by humans
Unstructured Data Analysis,Text,4,2,These approaches depend on semantic fea- tures that cannot be explicitly expressed through the text
Unstructured Data Analysis,Text,4,2,"The computational models are useful for theoretical purposes, e.g."
Unstructured Data Analysis,Text,4,2,Developing a program that understands natural language is a difficult prob- lem
Unstructured Data Analysis,Text,4,2,"The majority of natural languages are large, they contain (for all practical purposes) infinitely many sentences"
Unstructured Data Analysis,Text,4,2,Also there is much ambiguity in natural language
Unstructured Data Analysis,Text,4,2,"Many words have several meanings, such as “can,” “bear,” “fly,” “orange,” and the same sentences many times have different meanings in different contexts"
Unstructured Data Analysis,Text,4,2,The syntax of a language helps us to decide how the words are being combined to make larger meanings
Unstructured Data Analysis,Text,4,2,"For example, in the sentence “the dealer sold the merchant a dog,” it is important to be clear what is sold to whom"
Unstructured Data Analysis,Text,4,2,"Assuming that we are able to overcome these problems, we must then create an internal represen- tation to be able to use the information in an appropriate way"
Unstructured Data Analysis,Text,4,2,"This is a problem at the level of semantics and pragmatics, which is also a difficult and ambiguous problem"
Unstructured Data Analysis,Text,4,2,Consider the following sentences:
Unstructured Data Analysis,Text,4,2,NLP is the subject of computational linguistics—the study of computer systems for understand- ing and generating natural language
Unstructured Data Analysis,Text,4,2,"Linguistics has two branches, computational linguistics and theoretical linguistics"
Unstructured Data Analysis,Text,4,2,Computational linguistics deals with developing algorithms for handling a useful range of natural language as input
Unstructured Data Analysis,Text,4,2,"Theoretical linguistics focuses primarily on one as- pect of language performance: grammatical competence, or how people accept some sentences as correct, following grammatical rules, and others as incorrect in the grammatical sense"
Unstructured Data Analysis,Text,4,2,Theoreti- cal linguistics is concerned with language universals—principals of grammar which apply to all natural languages
Unstructured Data Analysis,Text,4,2,"More specifically, computational linguistics, and thus NLP, is concerned with the study of natural language analysis and language generation"
Unstructured Data Analysis,Text,4,2,Natural language analysis further divides into two domains: sentence analysis and discourse and dialogue structure
Unstructured Data Analysis,Text,4,2,"Currently, processing individual sentences is more advanced than determining discourse structure"
Unstructured Data Analysis,Text,4,2,"Fortunately, for many applications, thorough discourse analysis is not necessary and sentences can often be understood without a complete discourse analysis"
Unstructured Data Analysis,Text,4,2,Sentence analysis further divides into syntax analysis and semantic analysis
Unstructured Data Analysis,Text,4,2,The overall ob- jective of sentence analysis is to determine what a sentence “means
Unstructured Data Analysis,Text,4,2,"” In practice, this involves translating the natural language input into a language with simple semantics, for example, formal logic, or into a database command language"
Unstructured Data Analysis,Text,4,2,"In most systems, the first stage is syntax analysis"
Unstructured Data Analysis,Text,4,2,"So far, we have discussed the structure and meaning of individual sentences, but what what about the meaning of the entire text? The information conveyed by a text is clearly more than the sum of its parts; it is more than the meaning of its individual sentences"
Unstructured Data Analysis,Text,4,2,"For example, if a text tells a story, describes a procedure, or presents an argument, we must understand the con- nections between the component sentences in order to have fully understood the story"
Unstructured Data Analysis,Text,4,2,"The problem we face is, what does “it” stand for? There are four candidates in the first sen- tence: dawn, Vikrant, blue-ship, and torpedo"
Unstructured Data Analysis,Text,4,2,The semantic analysis helps to exclude “dawn” as a candidate for “it” and number agreement excludes “torpedoes
Unstructured Data Analysis,Text,4,2,"” But, there are still two candidates: the Vikrant and the blue-ship, which are both ships"
Unstructured Data Analysis,Text,4,2,Our syntax and semantic analysis tools will not enable us to conclude what “it” is between these two candidates
Unstructured Data Analysis,Text,4,2,"To arrive at the noun for “it,” we must understand the causal relationship between the firing of torpedoes and sinking of ship"
Unstructured Data Analysis,Text,4,2,"Since the Vikrant fired torpedoes, it must have fired on the blue-ship, since we can logically conclude that the Vikrant would not fire on itself"
Unstructured Data Analysis,Text,4,2,"So, “it” must be the blue-ship"
Unstructured Data Analysis,Text,4,2,The grammar of a language can be viewed as a theory of the structure of that language
Unstructured Data Analysis,Text,4,2,"Like any mathematical theory, the theory of grammar is based on a a set of a finite number of observed sentences, but it projects this to an infinite set of grammatical sentences"
Unstructured Data Analysis,Text,4,2,"This becomes possible by creating general laws (grammatical rules), framed in terms of such hypothetical constructs as the particular phonemes, words, and phrases, of the spoken language being analyzed"
Unstructured Data Analysis,Text,4,2,A properly for- mulated grammar should be able to unambiguously determine the set of all grammatical sentences in that language
Unstructured Data Analysis,Text,4,2,Suppose a large corpus x does not contain either of these sentences
Unstructured Data Analysis,Text,4,2,"In this scenario can we say, “Grammar that is determined for corpus x will direct the corpus to include the first sentence and exclude second"
Unstructured Data Analysis,Text,4,2,"”? Even though such simple cases maybe not adequate to confirm that all correct sentences of that language will be identified as correct and all wrong sentences will be rejected, but nevertheless we can still use this testing framework to identify grammatically correct sentences from incorrect ones"
Unstructured Data Analysis,Text,4,2,"A phrase structure grammar is defined using a finite vocabulary (alphabet) Σ; a finite set of variables V ; and a finite set P (rewrite rules) of productions of the form X→Y,whereXisastringinV andY inV ∪Σ"
Unstructured Data Analysis,Text,4,2,"Hence,agrammarisG=(V,Σ,S,P),where Σ is set of terminal symbols, which appear at the end of generation, and S is a start symbol (for sentence)"
Unstructured Data Analysis,Text,4,2,The corresponding language of G is L(G)
Unstructured Data Analysis,Text,4,2,A derivation of a sentence can then be thought of as a proof with V taken as axiom system and P as the rules of inferences
Unstructured Data Analysis,Text,4,2,"We say that L is a derivable language if L is the set of strings that are derivable from grammar G and we say that L is a terminal language if it is the set of terminal strings from some system (V, Σ, S, P )"
Unstructured Data Analysis,Text,4,2,This information can be loaded into a database and can be queried any number of times to find useful information
Unstructured Data Analysis,Text,4,2,"Preprocessing of the documents can be achieved by a variety of mod- ules such as text segmenters, filters, tokenizers, lexical analyzers, stemmers, and disambiguators."
Unstructured Data Analysis,Text,4,2,"Historically, to perform IE, a traditional ar- chitecture based on natural language understanding was used"
Unstructured Data Analysis,Text,4,2,This method requires full parsing of sentences followed by a semantic interpretation of syntactic structure obtained through parsing
Unstructured Data Analysis,Text,4,2,Discourse analysis was carried out as a final step
Unstructured Data Analysis,Text,4,2,"A new approach to IE offers a fresh perspec- tive from the traditional one just described, where only the concepts that are within the scope of extraction are required from the documents"
Unstructured Data Analysis,Text,4,2,"This leads to simplification of syntactic and seman- tic analysis due to a more restricted, deterministic, and collaborative process"
Unstructured Data Analysis,Text,4,2,"The new approach has the strategy that replaces the traditional parsing, interpretation, and discourse (module) with a simple phrasal parser (parsing based on phrases)"
Unstructured Data Analysis,Text,4,2,The latter finds the local phrases
Unstructured Data Analysis,Text,4,2,"In addition, the discourse module does the job of event pattern matching and merging the of templates"
Unstructured Data Analysis,Text,4,2,"The above approach is useful because full parsing is typically expensive and not robust, which produces ambiguous results and cannot manage off-vocabulary conditions"
Unstructured Data Analysis,Text,4,2,"The current IE systems make use of partial parsing, so the process of finding the constituent par- tial structure of a sentence consists of applying one or more cascaded steps onto the text fragments"
Unstructured Data Analysis,Text,4,2,The generated constituents are tagged as parts-of-speech—such as noun/verb—or phrases—such as prepositional or others
Unstructured Data Analysis,Text,4,2,"After parsing of the constituents, the system resolves domain-specific dependencies based on the semantic restrictions imposed by the extraction environment"
Unstructured Data Analysis,Text,4,2,The de- pendencies are resolved using the following two alternatives
Unstructured Data Analysis,Text,4,2,"Natural language question answering (NL-QA) derives common features from NLP and infor- mation retrieval (IR), but there is the subtlety that NL-QA aims to deliver “answers,” while IR usually delivers “hits"
Unstructured Data Analysis,Text,4,2,"” Most of NL-QA deals with focused questions based on facts, such as“Who invented the paper clip?” These kinds of questions are called factoid questions, which can be typi- cally answered using the named entities such as people, organizations, measures, dates, locations, etc"
Unstructured Data Analysis,Text,4,2,Data Redundancy Based Approach
Unstructured Data Analysis,Text,4,2,"The data redundancy based approach depends on statisti- cal regularities, from which it is possible to extract “easy” answers to factoid questions"
Unstructured Data Analysis,Text,4,2,"To answer a question, the system makes some connection between the question and the passages containing the answers"
Unstructured Data Analysis,Text,4,2,"Having done this connection at the lexical level, the rest of the job is simple"
Unstructured Data Analysis,Text,4,2,The meaning of the lexical connection between the question and likely answer carrying passage is the presence of a large degree of overlap between these texts
Unstructured Data Analysis,Text,4,2,"But, in reality, this is unfortunately not often the case in practice"
Unstructured Data Analysis,Text,4,2,"This is due to the richness of natural language and its expressive power, which means that varying textual forms can have the same meaning (semantics)"
Unstructured Data Analysis,Text,4,2,"But, this richness also provides a good degree of semantic resemblance between the question and answer-carrying text"
Unstructured Data Analysis,Text,4,2,"To appreciate this scenario, consider the following question which has two different answers"
Unstructured Data Analysis,Text,4,2,"These answers are lexically different, but have the same semantics"
Unstructured Data Analysis,Text,4,2,The problem of clustering has been studied widely in the statistics literature in the context of a wide variety of data mining tasks
Unstructured Data Analysis,Text,4,2,Clustering entails finding groups of similar objects in the data
Unstructured Data Analysis,Text,4,2,"The similarity between the objects is measured using some similarity function; often, a distance function is taken"
Unstructured Data Analysis,Text,4,2,"The problem of clustering can be very useful in the text domain, where the objects to be clusters can be of different granularities such as documents, paragraphs, sentences, or terms"
Unstructured Data Analysis,Text,4,2,Clustering is especially useful for organizing documents to improve retrieval and support browsing
Unstructured Data Analysis,Text,4,2,The study of the clustering problem precedes its applicability to the text domain
Unstructured Data Analysis,Text,4,2,"Traditional methods for clustering have generally focused on the case of quantitative data, where the attributes of the data are numerical"
Unstructured Data Analysis,Text,4,2,"The problem has also been studied for the case of categorical data, where attributes may take on nominal values"
Unstructured Data Analysis,Text,4,2,"A number of implementations of common text clustering algorithms are available in several toolkits, such as Lemur and BOW"
Unstructured Data Analysis,Text,4,2,"We note that many classes of algorithms—such as the k-means algorithm or hierarchical algorithms— are general-purpose methods, which can be extended to any kind of data, including text data"
Unstructured Data Analysis,Text,4,2,"Text clustering algorithms are divided into a wide variety of different types such as agglomerative clus- tering algorithms, partitioning algorithms, and standard parametric modeling based methods"
Unstructured Data Analysis,Text,4,2,Distance-based clustering algorithms are designed by using a similarity function that is a distance function or metric to measure the closeness between the text objects
Unstructured Data Analysis,Text,4,2,The most well known sim- ilarity function which is used commonly in the text domain is the cosine similarity function
Unstructured Data Analysis,Text,4,2,Let U = (f(u1)···f(uk)) and V = (f(v1)···f(vk)) be the damped and normalized frequency term vector in two different documents U and V
Unstructured Data Analysis,Text,4,2,"The values u1,"
Unstructured Data Analysis,Text,4,2,", vk represent the (normalized) term frequencies and the function f (·) represents the damping function"
Unstructured Data Analysis,Text,4,2,Some exam- ples of damping functions for f (·) are the square-root or the logarithm
Unstructured Data Analysis,Text,4,2,Computing text similarity is a fundamental problem in IR
Unstructured Data Analysis,Text,4,2,"Although most of the work in IR has focused on how to assess the similarity of a keyword query and a text document, rather than the similarity between two documents, many weighting heuristics and similarity functions can also be applied to optimize the similarity function for clustering"
Unstructured Data Analysis,Text,4,2,Effective IR models generally capture three heuristics: term-frequency (TF) weighting; inverse document frequency (IDF) weighting; and document length normalization
Unstructured Data Analysis,Text,4,2,"One effective way to assign weights to terms when represent- ing a document as a weighted term vector is the BM25 term weighting method, where the normal- ized TF not only addresses length normalization, but also has an upper bound which improves the robustness as it avoids overly rewarding the matching of any particular term"
Unstructured Data Analysis,Text,4,2,A document can also be represented with a probability distribution over words (ie
Unstructured Data Analysis,Text,4,2,", unigram language models) and the similarity can then be measured based an information theoretic measure such as cross entropy or Kullback–Leibler divergence"
Unstructured Data Analysis,Text,4,2,"For clustering, symmetric variants of such a similarity function may be more appropriate"
Unstructured Data Analysis,Text,4,2,One challenge in clustering short segments of text (eg
Unstructured Data Analysis,Text,4,2,", tweets or sentences) is that exact keyword matching may not work well"
Unstructured Data Analysis,Text,4,2,The general concept of agglomera- tive clustering is to successively merge documents into clusters based on their similarity with one another
Unstructured Data Analysis,Text,4,2,Almost all the hierarchical clustering algorithms successively merge groups based on the best pairwise similarity between these groups of documents
Unstructured Data Analysis,Text,4,2,The main differences between these classes of methods are how this pairwise similarity is computed between the different groups of documents
Unstructured Data Analysis,Text,4,2,"For example, the similarity between a pair of groups may be computed as the best-case similarity, average-case similarity, or worst-case similarity between documents which are drawn from these pairs of groups"
Unstructured Data Analysis,Text,4,2,"Single Linkage Clustering: Here, the similarity between two groups of documents is the greatest similarity between any pair of documents from these two groups"
Unstructured Data Analysis,Text,4,2,"In single linkage clustering, we merge the two groups where their closest pair of documents have the highest similarity compared to any other pair of groups"
Unstructured Data Analysis,Text,4,2,The main advantage of single linkage clustering is that it is extremely efficient to implement in practice
Unstructured Data Analysis,Text,4,2,This is because we can first compute all similarity pairs and sort them in order of reducing similarity
Unstructured Data Analysis,Text,4,2,These pairs are processed in this pre-defined order and the merge is performed successively if the pairs belong to different groups
Unstructured Data Analysis,Text,4,2,It can be easily shown that this approach is equivalent to the single-linkage method
Unstructured Data Analysis,Text,4,2,This is essentially equivalent to a spanning tree algorithm on the complete graph of pairwise distances by processing the edges of the graph in a certain order
Unstructured Data Analysis,Text,4,2,"Here, the similarity between two clusters is the aver- age similarity between the pairs of documents in the two clusters"
Unstructured Data Analysis,Text,4,2,"Average linkage clustering process is somewhat slower than single-linkage clustering, because we need to determine the average similarity between a large number of pairs in order to determine group-wise simi- larity"
Unstructured Data Analysis,Text,4,2,"On the other hand, it is much more robust in terms of clustering quality, because it does not exhibit the chaining behavior of single linkage clustering"
Unstructured Data Analysis,Text,4,2,Average linkage clus- tering algorithm can be made to run faster by approximating the average linkage similarity between two clusters C1 and C2 by computing the similarity between the mean document of C1 and the mean document of C2
Unstructured Data Analysis,Text,4,2,"While this approach does not work equally well for all data domains, it works particularly well for the case of text data"
Unstructured Data Analysis,Text,4,2,"Using this approach, the running time can be reduced to O(n2), where n is the total number of nodes"
Unstructured Data Analysis,Text,4,2,"The method can be implemented quite efficiently in the case of document data, because the centroid of a cluster is simply the concatenation of the documents in that cluster"
Unstructured Data Analysis,Text,4,2,"Here, the similarity between two clusters is the worst-case similarity between any pair of documents in the two clusters"
Unstructured Data Analysis,Text,4,2,Complete-linkage clustering can also avoid chaining because it avoids the placement of any pair of very disparate points in the same cluster
Unstructured Data Analysis,Text,4,2,"However, like group-average clustering, it is computationally more expensive than the single-linkage method"
Unstructured Data Analysis,Text,4,2,The complete linkage clustering method requires O(n2) space and O(n3) time
Unstructured Data Analysis,Text,4,2,"The space requirement can however be significantly lower in the case of the text data domain, because a large number of pairwise similarities are zero"
Unstructured Data Analysis,Text,4,2,"In k-medoid clustering algorithms, we use a set of points from the original data as the anchors (or medoids) around which the clusters are built"
Unstructured Data Analysis,Text,4,2,The key aim of the algorithm is to determine an optimal set of representative documents from the original corpus around which the clusters are built
Unstructured Data Analysis,Text,4,2,Each document is assigned to its closest repre- sentative from the collection
Unstructured Data Analysis,Text,4,2,This creates a running set of clusters from the corpus which are successively improved by a randomized process
Unstructured Data Analysis,Text,4,2,The algorithm works with an iterative approach where the set of k representatives are succes- sively improved with the use of randomized inter-changes
Unstructured Data Analysis,Text,4,2,"Specifically, we use the average similarity of each document in the corpus to its closest representative as the objective func- tion which needs to be improved during this interchange process"
Unstructured Data Analysis,Text,4,2,"At each iteration, we replace a randomly picked representative in the current set of medoids with a randomly cho- sen representative from the collection, if it improves the clustering objective function"
Unstructured Data Analysis,Text,4,2,This approach is applied until convergence is achieved
Unstructured Data Analysis,Text,4,2,"There are two main disadvantages of the use of k-medoids based clustering algorithms, one of which is specific to the case of text data"
Unstructured Data Analysis,Text,4,2,One general disadvantage of k-medoids clus- tering algorithms is that they require a large number of iterations in order to achieve conver- gence and are therefore quite slow
Unstructured Data Analysis,Text,4,2,This is because each iteration requires the computation of an objective function whose time requirement is proportional to the size of the underlying corpus
Unstructured Data Analysis,Text,4,2,The second key disadvantage is that k-medoid algorithms do not work very well for sparse data such as text
Unstructured Data Analysis,Text,4,2,This is because a large fraction of document pairs do not have many words in common and the similarities between such document pairs are small (and noisy) values
Unstructured Data Analysis,Text,4,2,"Therefore, a single document medoid often does not contain all the concepts required in order to effectively build a cluster around it"
Unstructured Data Analysis,Text,4,2,"This characteristic is specific to the case of the information retrieval domain, because of the sparse nature of the underlying text data"
Unstructured Data Analysis,Text,4,2,The k-means clustering algorithm also uses a set of k representa- tives around which the clusters are built
Unstructured Data Analysis,Text,4,2,"However, these representatives are not necessarily obtained from the original data and are refined somewhat differently than a k-medoids ap- proach"
Unstructured Data Analysis,Text,4,2,The simplest form of the k-means approach is to start off with a set of k seeds from the original corpus and assign documents to these seeds on the basis of closest similarity
Unstructured Data Analysis,Text,4,2,"At the next iteration, the centroid of the assigned points to each seed is used to replace the seed in the last iteration"
Unstructured Data Analysis,Text,4,2,"In other words, the new seed is defined in such a way that it is a better central point for this cluster"
Unstructured Data Analysis,Text,4,2,This approach is continued until convergence
Unstructured Data Analysis,Text,4,2,One of the advantages of the k-means method over the k-medoids method is that it requires an extremely small number of iterations in order to converge
Unstructured Data Analysis,Text,4,2,The main disadvantage of the k-means method is that it is still quite sensitive to the initial set of seeds picked during the clustering
Unstructured Data Analysis,Text,4,2,"Secondly, the centroid for a given cluster of documents may contain a large number of words"
Unstructured Data Analysis,Text,4,2,This will slow down the similarity calculations at the next iteration
Unstructured Data Analysis,Text,4,2,"For example, another lightweight clustering method such as an agglomerative clustering technique can be used in order to decide the initial set of seeds"
Unstructured Data Analysis,Text,4,2,A second method for improving the initial set of seeds is to use some form of partial supervision in the process of initial seed creation
Unstructured Data Analysis,Text,4,2,This form of partial supervision can also be helpful in creating clusters which are designed for particular application-specific criteria
Unstructured Data Analysis,Text,4,2,Let’s take a closer look at the first two steps in detail to show how text model and network model are combined together
Unstructured Data Analysis,Text,4,2,"Suppose we are given a network with node set V = {1,"
Unstructured Data Analysis,Text,4,2,", n}"
Unstructured Data Analysis,Text,4,2,"For each pair of ordered nodes (i,j), let sij, record the information of the link from node i to node j"
Unstructured Data Analysis,Text,4,2,"sij could either be {0,1},N+, or any nonnegative value depending on the type of the link"
Unstructured Data Analysis,Text,4,2,"If sij ̸=0,wesaythereisadirectionallinkfromnodeitonodej,ornodeicitesj(equivalently,node j is cited by node i)"
Unstructured Data Analysis,Text,4,2,Let E = {(i → j) | sij ̸= 0} denote all the directional links in the network
Unstructured Data Analysis,Text,4,2,"Each node i has an associated “link-in” space, denoted by LI(i) ∈ V, which is the set of nodes that could possibly cite node i"
Unstructured Data Analysis,Text,4,2,"Similarly, each node i is associated with a “link-out” space denoted by LO(i) ∈ V, which is the set of nodes that could possibly been cited by node i"
Unstructured Data Analysis,Text,4,2,"Although in most cases we have LI(i) = LO(i) = V, in some scenarios such as citation of publications, the link-out space of a paper is the set of all papers that are older than the paper itself, and the link-in space is the set of all papers that are newer than the paper itself"
Unstructured Data Analysis,Text,4,2,"Let I(i) = {j | sji ̸= 0} be the set of nodes that actually cite node i, O(i) = {j | sij ̸= 0} be the set of nodes that are actually cited by node i and din(i) = |I(i)| be the in-degree of node i, dout(i) = |O(i)| be the out-degree of node i"
Unstructured Data Analysis,Text,4,2,"Finally, we denote by K the number of communities we aim to find"
"Unsupervised Learning
",Dimension Reduction,4,2,"To motivate dimensionality reduction methods, we shall briefly review the phe- nomenon known as the curse of dimensionality, first identified by Bellman (1957) in the context of dynamic programming"
"Unsupervised Learning
",Dimension Reduction,4,2,"Suppose that our task is to analyse a data set consisting of d-dimensional samples x1,"
"Unsupervised Learning
",Dimension Reduction,4,2,",xn ‚àà Rd"
"Unsupervised Learning
",Dimension Reduction,4,2,The immediate difficulties that arise from large dimen- sionality d are: ‚ãÜ Difficulty of visualising the data(in terms of dependence and distribution)
"Unsupervised Learning
",Dimension Reduction,4,2,"ãÜ Costly storage of data, especially if both d and n are large"
"Unsupervised Learning
",Dimension Reduction,4,2,"But the issues that arise from large dimensionality may also be more subtle: ‚ãÜ Computational cost - many numerical algorithms are at least quadratic in computational complexity in terms of dimensionality, that is, the number of operations they require grows at least as fast as d2"
"Unsupervised Learning
",Dimension Reduction,4,2,(Multiplication of square matrices and solving systems of linear equations are examples
"Unsupervised Learning
",Dimension Reduction,4,2,") Then the impact of having, say, d = 100 versus d = 10 can be significant, if not enormous"
"Unsupervised Learning
",Dimension Reduction,4,2,Sometimes large dimensionality may prevent us from using a particular method altogether: Example 1
"Unsupervised Learning
",Dimension Reduction,4,2,"If the dimensionality d exceeds the number of samples, n, then it is possible to show that the sample covariance matrix of x1,"
"Unsupervised Learning
",Dimension Reduction,4,2,",xn is not invertible"
"Unsupervised Learning
",Dimension Reduction,4,2,"Any method that requires the inverse of the sample covariance matrix, which is known as the sample precision matrix, becomes inapplicable"
"Unsupervised Learning
",Dimension Reduction,4,2,"(For example, weighted least-squares estimation"
"Unsupervised Learning
",Dimension Reduction,4,2,) 1
"Unsupervised Learning
",Dimension Reduction,4,2,"2 Intrinsic dimensionality and latent variables When working with multivariate data sets, we often observe that some of the vari- ables in the data set are: ‚ãÜ  irrelevant - they do not contain useful information concerning the prob- lem we are studying (they may be just ""noise""), ‚ãÜ  redundant - they contain information that is already conveyed by other variables in the data set"
"Unsupervised Learning
",Dimension Reduction,4,2,1 M
"Unsupervised Learning
",Dimension Reduction,4,2,It is also possible that the observed variables are governed by a smaller set of unobservable latent variables
"Unsupervised Learning
",Dimension Reduction,4,2,"In dimensionality reduction, we are thus often interested in determining the intrinsic dimensionality of the data set, which is the dimensionality of a minimal set of variables that describe the essential information and variation present in the data"
"Unsupervised Learning
",Dimension Reduction,4,2,"While determining the intrinsic dimensionality, we seek to remove the irrelevant and redundant variables and/or to recover the underlying latent vari- ables"
"Unsupervised Learning
",Dimension Reduction,4,2,"Principal component analysis (PCA) is a method to linearly transform multivari- ate data to new orthogonal variables, called principal components (PCs)"
"Unsupervised Learning
",Dimension Reduction,4,2,"While fundamentally it is a data transformation/rotation method, it lends itself to di- mensionality reduction when we discard some of the less important principal components"
"Unsupervised Learning
",Dimension Reduction,4,2,We will discuss this in Section 2
"Unsupervised Learning
",Dimension Reduction,4,2,PCA was invented by Pearson (1901) and later independently re-discovered by Hotelling (1933)
"Unsupervised Learning
",Dimension Reduction,4,2,"There are many formulations of PCA, the most common of which is perhaps the maximum variance formulation, which we look into first"
"Unsupervised Learning
",Dimension Reduction,4,2,"To this end, consider d-dimensional data consisting of samples xi ∈Rd,i=1,"
"Unsupervised Learning
",Dimension Reduction,4,2,1 Their sample mean and covariance matrix are defined by then 1nTT2TV(c):= 􏰁􏰉c xi−c x􏰊 =c Σc∈R≥0
"Unsupervised Learning
",Dimension Reduction,4,2,"1nd1n Td×d x:=􏰁xi∈R,Σ:=􏰁􏰉xi−x􏰊􏰉xi−x􏰊∈R ,n i=1 n i=1respectively"
"Unsupervised Learning
",Dimension Reduction,4,2,"2 A generic vector c ∈ Rd defines a projection of the data via c T x i ∈ R, i = 1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",n"
"Unsupervised Learning
",Dimension Reduction,4,2,"The sample variance of the projected data cT xi ∈ R, i = 1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",n, isn i=1It does not make sense to maximise V (c ) over all c ∈ Rd — we can typically makeV (c ) arbitrarily large by taking ∥c ∥ → ∞"
"Unsupervised Learning
",Dimension Reduction,4,2,"Hence, we introduce a normalising constraint ∥c ∥ = 1, which is equivalent to ∥c ∥2 = c T c = 1"
"Unsupervised Learning
",Dimension Reduction,4,2,The constrained maximi- sation problemargmaxV (c) such that cT c = 1 (2
"Unsupervised Learning
",Dimension Reduction,4,2,"1) c ∈Rdcan be solved using the Lagrangian function L(c,λ):=V(c)+λ(1−cTc)=cTΣc+λ(1−cTc)with Lagrange multiplier λ ∈ R"
"Unsupervised Learning
",Dimension Reduction,4,2,"Differentiating L (c , λ) with respect to c , i"
"Unsupervised Learning
",Dimension Reduction,4,2,e
"Unsupervised Learning
",Dimension Reduction,4,2,", working out the gradient∇cL(c,λ)=2Σc−2λc,1Application of PCA may become problematic if the dimensionality d is approximately equal to or exceeds the number of samples n"
"Unsupervised Learning
",Dimension Reduction,4,2,"For more information, see, e"
"Unsupervised Learning
",Dimension Reduction,4,2,g
"Unsupervised Learning
",Dimension Reduction,4,2,", Johnstone and Paul (2018)"
"Unsupervised Learning
",Dimension Reduction,4,2,"2Alternatively, normalisation by 1 instead of 1 could be used for the sample covariancen−1 nmatrix but, apart from cases where n is small, the difference is insignificant in practice"
"Unsupervised Learning
",Dimension Reduction,4,2,1M
"Unsupervised Learning
",Dimension Reduction,4,2,"we find that a maximiser (c1,λ1) of the Lagrangian must necessarily solve ∇cL(c1,λ1)=0 ⇐⇒ Σc1 =λ1c1"
"Unsupervised Learning
",Dimension Reduction,4,2,"Hence, c1 is an eigenvector of Σ, and λ1 is the corresponding eigenvalue"
"Unsupervised Learning
",Dimension Reduction,4,2,"Subse- quently, such a maximiser c1 defines the first PC of the data (with centring) via cT1 (xi −x),i =1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,Note that the sample variance of the first PC is V(c1)=cT1 Σc1 =λ1cT1 c1 =λ1
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,"The vectors c2,cd determining the remaining PCs are defined similarly in an inductive fashion"
"Unsupervised Learning
",Dimension Reduction,4,2,"Suppose that we have already worked out the vectors c1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,"Then cj is obtained as amaximiser of the sample variance V (c ) under the constraint c T c = 1 but with an additional constraint that c is orthogonal to the vectors c1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,"Thatis,cj solvesc T c = 1 􏰇 􏰆􏰅 􏰈normalisationsubspace of Rd as j increases"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,"3)􏰇􏰆􏰅􏰈 􏰇􏰆􏰅􏰈 􏰇􏰆􏰅􏰈=λ1 =λ2 =λdThis indicates that the first PC has the largest sample variance, the second PC hasthe second largest sample variance and so on"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,"Since we know that c1,,cd are eigenvectors of Σ and λ1, ,λd are the matching eigenvalues, to perform PCA in practice, we do not have to solve (2"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,1) or (2
"Unsupervised Learning
",Dimension Reduction,4,2,2) numerically
"Unsupervised Learning
",Dimension Reduction,4,2,"Instead, we can use numerical eigendecomposi- tion algorithms, or the singular value decomposition (SVD), to find c1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",cd and λ1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",λd"
"Unsupervised Learning
",Dimension Reduction,4,2,Remark 2
"Unsupervised Learning
",Dimension Reduction,4,2,5
"Unsupervised Learning
",Dimension Reduction,4,2,"The entries of the vectors c1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",cd are sometimes called loadings"
"Unsupervised Learning
",Dimension Reduction,4,2,"However, this term is not used consistently, as sometimes it is also used to re- fer to the entries of 􏰄λ1c1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",􏰄λdcd"
"Unsupervised Learning
",Dimension Reduction,4,2,"In these lectures, we adopt the former interpretation if we use the word “loading”"
"Unsupervised Learning
",Dimension Reduction,4,2,arg max V (c ) c ∈Rdsuch thatand c T c k = 0 for any k < j
"Unsupervised Learning
",Dimension Reduction,4,2,􏰇 􏰆􏰅 􏰈(2
"Unsupervised Learning
",Dimension Reduction,4,2,2)  orthogonalityThis optimisation problem can again be solved using a Lagrangian function
"Unsupervised Learning
",Dimension Reduction,4,2,It thentranspiresthatcj isalsoaneigenvectorofΣandthattheLagrangemultiplier λ j for the normalisation constraint c T c = 1 is the matching eigenvalue
"Unsupervised Learning
",Dimension Reduction,4,2,"The j -th PCisthendefinedascTj (xi −x),i =1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",n"
"Unsupervised Learning
",Dimension Reduction,4,2,"Moreover,V(cj)=λj"
"Unsupervised Learning
",Dimension Reduction,4,2,Notethatthe orthogonality constraint c T c k = 0 for any k < j is the more restrictive the larger j is
"Unsupervised Learning
",Dimension Reduction,4,2,"Thus, we are seeking the maximiser c j of V (c ) from a progressively smaller2M"
"Unsupervised Learning
",Dimension Reduction,4,2,Pakkanen Unsupervised Learning Autumn 2023 2
"Unsupervised Learning
",Dimension Reduction,4,2,"2 Dimensionality reductionAs mentioned in passing above, we can use PCA for dimensionality reduction by discarding PCs deemed negligible"
"Unsupervised Learning
",Dimension Reduction,4,2,"In the context of PCA, PCs with low enough sample variance may be regarded as insignificant"
"Unsupervised Learning
",Dimension Reduction,4,2,In view of (2
"Unsupervised Learning
",Dimension Reduction,4,2,"3), we select q = 1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",d − 1 such that the sample variance V (cq+1) = λq+1 of the q + 1-st PC is small enough"
"Unsupervised Learning
",Dimension Reduction,4,2,We then retain the PCs up to the q-th PC (inclusive) and discard the rest
"Unsupervised Learning
",Dimension Reduction,4,2,"While intuitively plausible, this procedure can also be formalised via reconstruction error minimisation as follows"
"Unsupervised Learning
",Dimension Reduction,4,2,"Suppose that we seek to approximate the original samples x1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",xn by new vectors xˆ1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",xˆn in Rd which take specific form,xˆi :=μ+Uqzi,where μ ∈ Rd is a location vector, Uq ∈ Rd×q is a matrix whose columns are or- thogonal vectors with unit norm and zi ∈ Rq for any i = 1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",n"
"Unsupervised Learning
",Dimension Reduction,4,2,Then we seek to minimise the (mean-square) reconstruction error1􏰁n 21􏰁n 2n ∥xi −xˆi∥ = n ∥xi −μ−Uqzi∥ (2
"Unsupervised Learning
",Dimension Reduction,4,2,"6)i=1 i=1overμ,Uq andz1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",zn satisfyingtheaboverequirements"
"Unsupervised Learning
",Dimension Reduction,4,2,"So,effectivelyweare trying to recover x1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",xn optimally from the lower-dimensional data z1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",zn"
"Unsupervised Learning
",Dimension Reduction,4,2,"It can be shown (Bishop, 2006, Sect"
"Unsupervised Learning
",Dimension Reduction,4,2,12
"Unsupervised Learning
",Dimension Reduction,4,2,1
"Unsupervised Learning
",Dimension Reduction,4,2,2) that (2
"Unsupervised Learning
",Dimension Reduction,4,2,"6) is minimised by1 􏰁nμ = x = nU =􏰂c ···c 􏰃xi ,zi=UqT(xi−x), i=1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",n,where we interpret all vectors as column vectors"
"Unsupervised Learning
",Dimension Reduction,4,2,"Note that the j -th component of zi (for j ∈ 1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",q) is the j-th PC of the i-th sample, i"
"Unsupervised Learning
",Dimension Reduction,4,2,e
"Unsupervised Learning
",Dimension Reduction,4,2,", cTj (xi −x)"
"Unsupervised Learning
",Dimension Reduction,4,2,"Thus, the first q PCs provide an optimal q-dimensional description of the original data in the above sense"
"Unsupervised Learning
",Dimension Reduction,4,2,2
"Unsupervised Learning
",Dimension Reduction,4,2,"3 Selecting dimensionalityThe number q of PCs for the lower-dimensional description of the data is usually chosen based on an analysis of the sample variances of the PCs, i"
"Unsupervised Learning
",Dimension Reduction,4,2,e
"Unsupervised Learning
",Dimension Reduction,4,2,", V (c j ) = λ j , j = 1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,", d"
"Unsupervised Learning
",Dimension Reduction,4,2,"Specifically, we can compare them to the total variance given by thei=1 q1q3M"
"Unsupervised Learning
",Dimension Reduction,4,2,"Pakkanen Unsupervised Learning Autumn 2023 sum of the variances of all PCs, Plotting the ratiodd􏰁 V (c j ) = 􏰁 λj"
"Unsupervised Learning
",Dimension Reduction,4,2,"j=1 j=1λj , 􏰀dj′=1λj′ interpreted as the proportion of variance explained by the j -th PC, across all j = 1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",d, yields the scree plot"
"Unsupervised Learning
",Dimension Reduction,4,2,"Alternatively, we can also look at the cumulative variance of the first q PCs via the ratio􏰀qj=1λj , q=1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",d, 􏰀dj′=1λj′and choose q such that the ratio exceeds a level deemed sufficiently high, e"
"Unsupervised Learning
",Dimension Reduction,4,2,g
"Unsupervised Learning
",Dimension Reduction,4,2,", 95%"
"Unsupervised Learning
",Dimension Reduction,4,2,A well-known limitation of principal component analysis (PCA) is that the prin- cipal components may be hard to interpret since they are linear combinations of all input variables (apart from some exceptional cases)
"Unsupervised Learning
",Dimension Reduction,4,2,A pragmatic and oft- used solution is to simply adjust small loadings in the PCA eigenvectors to zero
"Unsupervised Learning
",Dimension Reduction,4,2,"But such an approach is open to criticism for being ad hoc and arbitrary — after all, how do we decide what is small?Sparse principal component analysis (SPCA), introduced by Zou et al"
"Unsupervised Learning
",Dimension Reduction,4,2,"(2006), offers a more principled approach to finding principal components with spar- sity, i"
"Unsupervised Learning
",Dimension Reduction,4,2,e
"Unsupervised Learning
",Dimension Reduction,4,2,", with many loadings typically shrunk to zero"
"Unsupervised Learning
",Dimension Reduction,4,2,SPCA could be based on PCA with L1 penalty similar to the lasso regression seen in supervised learning
"Unsupervised Learning
",Dimension Reduction,4,2,"(This is the SCoTLASS approach that precedes SPCA, devel- oped by Jolliffe et al"
"Unsupervised Learning
",Dimension Reduction,4,2,-2003
"Unsupervised Learning
",Dimension Reduction,4,2,") Instead, Zou et al"
"Unsupervised Learning
",Dimension Reduction,4,2,(2006) have built SPCA on the more general elastic net penalty which is a linear combination L1 (lasso) and L2 (ridge) penalties
"Unsupervised Learning
",Dimension Reduction,4,2,According to Hastie et al
"Unsupervised Learning
",Dimension Reduction,4,2,"(2009, p"
"Unsupervised Learning
",Dimension Reduction,4,2,"551), the function F(Θ,U) is not jointly convex in both Θ and U , which would impede the practical minimisation of F(Θ,U)"
"Unsupervised Learning
",Dimension Reduction,4,2,"However, both Θ 􏰞→ F(Θ,U∗) and U 􏰞→ F(Θ∗,U) are convex for fixed U∗ and Θ∗, respectively"
"Unsupervised Learning
",Dimension Reduction,4,2,"Hence, the minimisation of F(Θ,U) is done iteratively by alternatingly minimising Θ 􏰞→ F (Θ,U ∗) and U 􏰞→ F (Θ∗,U ), where U ∗ and Θ∗ are the respective previous minimisers, until convergence is reached"
"Unsupervised Learning
",Dimension Reduction,4,2,"(In fact, minimisation with respect to Θ can be obtained from singular value decomposition"
"Unsupervised Learning
",Dimension Reduction,4,2,)The classical PCA is inherently a linear method
"Unsupervised Learning
",Dimension Reduction,4,2,"However, many multivariate data sets encountered in practice exhibit non-linear relationships between the variables, which motivates the quest for non-linear extensions of PCA"
"Unsupervised Learning
",Dimension Reduction,4,2,"The first non-linear extension of PCA we study here is kernel principal com- ponent analysis, developed by Schölkopf et al"
"Unsupervised Learning
",Dimension Reduction,4,2,-1997
"Unsupervised Learning
",Dimension Reduction,4,2,"Kernel PCA builds on the idea that we can map the data x1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",xn ∈ Rd to a typically higher dimensional2 feature space Rp using a feature map f : Rd → Rp and then perform classical PCA on the transformed data f (x1),"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,", f (xn) in Rp"
"Unsupervised Learning
",Dimension Reduction,4,2,"However, since the feature mapf may require high dimensionality p to effectively linearise the non-linearities of the data, it is often desirable to avoid working directly with f (x1),"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,", f (xn)"
"Unsupervised Learning
",Dimension Reduction,4,2,"For- tunately, this is achievable using the so-called kernel trick, which is the basis of kernel learning methods"
"Unsupervised Learning
",Dimension Reduction,4,2,"To formulate kernel PCA, following Schölkopf et al"
"Unsupervised Learning
",Dimension Reduction,4,2,"(1997), consider first data x1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",xn andfeaturemap f suchthat n1 􏰑ni=1 f(xi)=0"
"Unsupervised Learning
",Dimension Reduction,4,2,(Removingthiscentred- ness assumption is not entirely trivial
"Unsupervised Learning
",Dimension Reduction,4,2,"vector c ∈ Rp would be an eigenvector of Σf , that is"
"Unsupervised Learning
",Dimension Reduction,4,2,Substituting (3
"Unsupervised Learning
",Dimension Reduction,4,2,2) and (3
"Unsupervised Learning
",Dimension Reduction,4,2,4) into (3
"Unsupervised Learning
",Dimension Reduction,4,2,"5), after some algebra, we find that (3"
"Unsupervised Learning
",Dimension Reduction,4,2,5) is equivalent to the equation which we seek to solve for α and positive λ > 0
"Unsupervised Learning
",Dimension Reduction,4,2,We note that if α solves theeigenvalue problem
"Unsupervised Learning
",Dimension Reduction,4,2,"Suppose now that c j ∈ Rp corresponds to the j -th kernel principal component (PC), representable via (3"
"Unsupervised Learning
",Dimension Reduction,4,2,"4) using vector α = (α1 ,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,", αn )"
"Unsupervised Learning
",Dimension Reduction,4,2,Then the projection of input x ∈ Rd onto this kernel PC is
"Unsupervised Learning
",Dimension Reduction,4,2,"The kernel trick stems from the observation that all the equations we need in order to perform kernel PCA, that is, (3"
"Unsupervised Learning
",Dimension Reduction,4,2,"7), (3"
"Unsupervised Learning
",Dimension Reduction,4,2,8) and (3
"Unsupervised Learning
",Dimension Reduction,4,2,"9), only depend on the feature map f through scalar products of the form f (x)T f (y)"
"Unsupervised Learning
",Dimension Reduction,4,2,This makes it possible to avoid working directly with the feature map by specifying f implicitly4 using a kernel function k
"Unsupervised Learning
",Dimension Reduction,4,2,"The advantage of the kernel trick is that we do not have to work in the potentially high-dimensional feature space Rp , involving a p × p matrix"
"Unsupervised Learning
",Dimension Reduction,4,2,"However, this ad- vantage is diminished if the number of samples, n, is very large since we need to deal with the n × n Gram matrix K under the kernel trick"
"Unsupervised Learning
",Dimension Reduction,4,2,"In practice, we use standard kernels, such as"
"Unsupervised Learning
",Dimension Reduction,4,2,Kernel PCA is useful for low-dimensional feature construction (e
"Unsupervised Learning
",Dimension Reduction,4,2,g
"Unsupervised Learning
",Dimension Reduction,4,2,", for classification), but one cannot reconstruct the original data from the kernel PCs"
"Unsupervised Learning
",Dimension Reduction,4,2,"The first obstacle is that after performing the kernel trick we do not have access to the feature map f , which we would need to invert in order to map the kernel PCs in the feature space Rp back to the input space Rd"
"Unsupervised Learning
",Dimension Reduction,4,2,"The second ob- stacle is that even if f were explicitly known, the kernel PCs would possibly lie outside the image of f , i"
"Unsupervised Learning
",Dimension Reduction,4,2,e
"Unsupervised Learning
",Dimension Reduction,4,2,", f (Rd) := {f (x) : x ∈ Rd} ⊂ Rp, making inversion impossible"
"Unsupervised Learning
",Dimension Reduction,4,2,"An autoencoder (Bourlard and Kamp, 1988; Kramer, 1991) is another non-linear extension of PCA"
"Unsupervised Learning
",Dimension Reduction,4,2,"Unlike kernel PCA, it approaches dimensionality reduction from the point of view of reconstruction error minimisation"
"Unsupervised Learning
",Dimension Reduction,4,2,"Autoencoders, which are sometimes also called autoassociative neural networks (Bishop, 2006, Sect"
"Unsupervised Learning
",Dimension Reduction,4,2,12
"Unsupervised Learning
",Dimension Reduction,4,2,4
"Unsupervised Learning
",Dimension Reduction,4,2,"2), are in the purview of deep learning, so we will only give here a high-level overview"
"Unsupervised Learning
",Dimension Reduction,4,2,"Givensamplesx1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",xn ∈Rd,an auto encoder first maps the data into a lower-dimensional space Rq using a function E : Rd → Rq called the encoder"
"Unsupervised Learning
",Dimension Reduction,4,2,"(That is, the dimensionality q is supposed to be much lower than d"
"Unsupervised Learning
",Dimension Reduction,4,2,) Then the trans- formed data are mapped from Rq back into Rd using another function D : Rq → Rd called the decoder
"Unsupervised Learning
",Dimension Reduction,4,2,"The objective is that the output D(E(xi)) is as close as possible to the input xi for any i = 1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",n"
"Unsupervised Learning
",Dimension Reduction,4,2,The encoder E = Eφ and decoder D = Dθ are set up as feedforward neural networks with their respective parameter vectors φ and θ
"Unsupervised Learning
",Dimension Reduction,4,2,The parameters in φ and θ (neural network weights and biases) are trained by minimising the reconstruction error using gradient descent (or minimising its mini-batch version using stochastic gradient descent)
"Unsupervised Learning
",Dimension Reduction,4,2,"If the decoder D is constrained to be linear and if the architecture of the encoder E covers all linear maps from Rd to Rq , then the autoencoder be- comes equivalent to classical PCA"
"Unsupervised Learning
",Dimension Reduction,4,2,"Canonical correlation analysis (CCA), developed by Hotelling (1936), is an exten- sion of principal component analysis which applies to two multivariate data sets with concurrent (but not necessarily equidimensional) samples"
"Unsupervised Learning
",Dimension Reduction,4,2,It aims to find projections of the two data sets that have maximal correlation
"Unsupervised Learning
",Dimension Reduction,4,2,Their sample cross-covariance matrix is defined as
"Unsupervised Learning
",Dimension Reduction,4,2,The orthogonality condition (4
"Unsupervised Learning
",Dimension Reduction,4,2,"1) will ensure that the sample correlation between the canonical variables uk,i , i = 1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",n, and uj,i , i = 1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",n, is zero for any j < k, and (4"
"Unsupervised Learning
",Dimension Reduction,4,2,"2) is an analogous requirement for vk,i, i = 1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",n, and vj,i, i = 1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",n"
"Unsupervised Learning
",Dimension Reduction,4,2,Since the constraints (4
"Unsupervised Learning
",Dimension Reduction,4,2,1) and (4
"Unsupervised Learning
",Dimension Reduction,4,2,"2) become stricter as k increases, it follows that"
"Unsupervised Learning
",Dimension Reduction,4,2,The constraints (4
"Unsupervised Learning
",Dimension Reduction,4,2,1) and (4
"Unsupervised Learning
",Dimension Reduction,4,2,"2) are the reason why we cannot find more than min{d,e} pairs of canonical variables"
"Unsupervised Learning
",Dimension Reduction,4,2,"We can find at most d orthog- onal components from x1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",xn ∈ Rd and at most d orthogonal components from y1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",yn ∈Re"
"Unsupervised Learning
",Dimension Reduction,4,2,We outline here the derivation of canonical variables
"Unsupervised Learning
",Dimension Reduction,4,2,"Like principal compo-nents, canonical variables could be solved using the Lagrange multiplier method,but we follow here a somewhat more direct approach, cf"
"Unsupervised Learning
",Dimension Reduction,4,2,Wikipedia (2023)
"Unsupervised Learning
",Dimension Reduction,4,2,"Tothis end, we first introduce the symmetric square roots Σ1/2 and Σ1/2 of the co- xyvariance matrices Σ and Σ and their (symmetric) inverses Σ−1/2 and Σ−1/2"
"Unsupervised Learning
",Dimension Reduction,4,2,2 We xy xycan then re-parameterise the problem using
"Unsupervised Learning
",Dimension Reduction,4,2,"Note that, thanks to the normalising denominator, the norms of a􏰣 and b􏰣 become immaterial, only the angles of the two vectors matter"
"Unsupervised Learning
",Dimension Reduction,4,2,"By the Cauchy–Schwarz inequality, where equality holds when the vectors Σ−1/2ΣT Σ−1/2a􏰣 and b􏰣 are linearly depen-dent (i"
"Unsupervised Learning
",Dimension Reduction,4,2,e
"Unsupervised Learning
",Dimension Reduction,4,2,", one is a scalar multiple of the other)"
"Unsupervised Learning
",Dimension Reduction,4,2,"Therefore, to maximise the left- hand side and ensure equality, we take"
"Unsupervised Learning
",Dimension Reduction,4,2,"Since the norm of the vector a􏰣1 does not affect ρ, we 􏰄􏰄can (and, in fact, we must) normalise it, i"
"Unsupervised Learning
",Dimension Reduction,4,2,e
"Unsupervised Learning
",Dimension Reduction,4,2,", 􏰄a􏰣1􏰄 = 1"
"Unsupervised Learning
",Dimension Reduction,4,2,"Like in principal com-ponent analysis, under this constraint the optimal a􏰣1 is then the eigenvector ofΣ−1/2Σ Σ−1ΣT Σ−1/2 corresponding to its largest eigenvalue"
"Unsupervised Learning
",Dimension Reduction,4,2,"Non-negative matrix factorisation (NMF) is an unsupervised learning technique that seeks to find approximate factorisation of a data matrix X ∈ Rd×n in terms of matrices W ∈ Rd×q and H ∈ Rq×n where≥0 ≥0≥0q ≤ max{d,n}, but ideally q much smaller than d and n"
"Unsupervised Learning
",Dimension Reduction,4,2,"The data matrix could be built froms amples x1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",xn ∈Rd"
"Unsupervised Learning
",Dimension Reduction,4,2,"For example, in a common application of NMF, X would be a (monochrome) image"
"Unsupervised Learning
",Dimension Reduction,4,2,"Then we can think of hi as a lower-dimensional de- scription of the sample xi for any i = 1,"
"Unsupervised Learning
",Dimension Reduction,4,2,",n, while W describes the linear map that aims to reconstruct xi from hi"
"Unsupervised Learning
",Dimension Reduction,4,2,"Thus, hi is analogous to the vector zi in the reconstruction error minimisation formulation of PCA, which contains the data on the principal components for the i -th sample"
"Unsupervised Learning
",Dimension Reduction,4,2,"As the name suggests, the non-negativity of the entries of W and H is key"
"Unsupervised Learning
",Dimension Reduction,4,2,"Preserving non-negativity is desirable in certain applications where the data are inherently non-negative, such as imaging"
"Unsupervised Learning
",Dimension Reduction,4,2,The non-negativity of W and H can also be helpful as far as interpretability is concerned
"Unsupervised Learning
",Dimension Reduction,4,2,"NMF provides an approximate decomposition of the data matrix X into additive components that cannot cancel each other, unlike principal components, which assume both positive and negative values"
"Unsupervised Learning
",Dimension Reduction,4,2,"To formalise NMF, we need to determine how well the approximation (6"
"Unsupervised Learning
",Dimension Reduction,4,2,1) holds
"Unsupervised Learning
",Dimension Reduction,4,2,"As discussed by Lee and Seung (2000), one could use either the squared Euclidean distance between matrices X and W H , given by or the quasi-likelihood function where the summand is the log-likelihood contribution of the observation Xi,j if it were Poisson-distributed with mean (W H )i , j"
"Unsupervised Learning
",Dimension Reduction,4,2,"Following Lee and Seung (1999), we adopt here the latter, L(W,H)"
"Unsupervised Learning
",Dimension Reduction,4,2,"Neither L(W,H) nor the Euclidean distance (6"
"Unsupervised Learning
",Dimension Reduction,4,2,3) can be optimised analytically with respect to W and H
"Unsupervised Learning
",Dimension Reduction,4,2,"To numerically maximise L(W,H) ,LeeandSeung (2000) develop an iterative algorithm where the following updates are performed alternatingly until convergence is achieved"
"Unsupervised Learning
",Dimension Reduction,4,2,"Moreover, initial values for W and H must be specified"
"Unsupervised Learning
",Dimension Reduction,4,2,"Packages that implement NMF generally offer various choices, covering both data-driven and randomised initialisers"
"Unsupervised Learning
",Dimension Reduction,4,2,"Neither L(W,H) nor the Euclidean distance (6"
"Unsupervised Learning
",Dimension Reduction,4,2,"3) are convex in W and H , which is why iterative algorithms may only converge to a local optimum"
"Unsupervised Learning
",Dimension Reduction,4,2,"Hence, the choice of (and experimentation with) initial values becomes important"
"Unsupervised Learning
",Dimension Reduction,4,2,"Moreover, the factorisation need not be unique even in the ideal case X = W H"
"Unsupervised Learning
",Dimension Reduction,4,2,"Independent component analysis (ICA) is a feature extraction technique motivated by the so-called cocktail-party problem,1 which can be described as follows"
"Unsupervised Learning
",Dimension Reduction,4,2,Suppose that two people are speaking simultaneously in an otherwise silent room
"Unsupervised Learning
",Dimension Reduction,4,2,Sound is then recorded through two microphones placed in different lo- cations in the room
"Unsupervised Learning
",Dimension Reduction,4,2,"Let us denote the two speakers’ speech signals by S1(t) and S2(t), respectively"
"Unsupervised Learning
",Dimension Reduction,4,2,"Ignoring acoustics (delay, reverberation) and possible imperfections in audio recording (distortion, noise etc), the signals X1(t) and X2(t) recorded through the two microphones can be expressed as linear mixtures with some weights a1,1, a1,2, a2,1, a2,2 ∈ R that depend on the locations of the speak- ers and microphones but are unknown"
"Unsupervised Learning
",Dimension Reduction,4,2,The problem is then about extracting the individual speech signals S1(t) and S2(t) from the recordings X1(t) and X2(t)
"Unsupervised Learning
",Dimension Reduction,4,2,The cocktail-party problem is generally not solvable unless further assumptions are made
"Unsupervised Learning
",Dimension Reduction,4,2,This is due to lack of identifiability — there may be infinitely many ways to express X1(t) and X2(t) in the form (6
"Unsupervised Learning
",Dimension Reduction,4,2,1)
"Unsupervised Learning
",Dimension Reduction,4,2,"To make the problem tractable, we introduce a probabilistic framework where S1(t) and S1(t) random variables"
"Unsupervised Learning
",Dimension Reduction,4,2,"For simplicity, we drop the time index t and simply write S1 and S2"
"Unsupervised Learning
",Dimension Reduction,4,2,"Then, the observed X1 and X2 are random variables satisfying in matrix form"
"Unsupervised Learning
",Dimension Reduction,4,2,The key assumption that makes the problem solvable and which ICA builds on is that the components S1 and S2 are mutually independent
"Unsupervised Learning
",Dimension Reduction,4,2,"Such an assumption is reasonable in the context of the cocktail-party problem, but also is not too restrictive in feature extraction in general"
"Unsupervised Learning
",Dimension Reduction,4,2,"Another key assumption behind ICA is that the independent components S1 and S2 are non-Gaussian, i"
"Unsupervised Learning
",Dimension Reduction,4,2,e
"Unsupervised Learning
",Dimension Reduction,4,2,", not normally distributed"
"Unsupervised Learning
",Dimension Reduction,4,2,"To see why this is necessary,2 suppose that S1 and S2 are standard normal and mutually independent"
"Unsupervised Learning
",Dimension Reduction,4,2,"Hence, the representation of X as a linear transformation of standard normal random variables is not unique"
"Unsupervised Learning
",Dimension Reduction,4,2,This would again amount to lack of identifiability and will be ruled out by assuming non-Gaussianity
"Unsupervised Learning
",Dimension Reduction,4,2,"It is not straightforward to pinpoint who invented ICA, as it has appeared in var- ious guises (and under different names) in the literature throughout the 1980s and 1990s (Comon, 1994)"
"Unsupervised Learning
",Dimension Reduction,4,2,Our formulation of ICA here follows Hastie et al
"Unsupervised Learning
",Dimension Reduction,4,2,"(2009, Sect"
"Unsupervised Learning
",Dimension Reduction,4,2,14
"Unsupervised Learning
",Dimension Reduction,4,2,7
"Unsupervised Learning
",Dimension Reduction,4,2,2) and Hyvärinen and Oja (2000)
"Unsupervised Learning
",Dimension Reduction,4,2,ICA builds on a probabilistic model where a d -dimensional random vector X satisfies for some matrix A ∈ Rd×d and a d-dimensional random vector S consisting of mutually independent components with zero mean and unit variance
"Unsupervised Learning
",Dimension Reduction,4,2,It is standard to assume that X has been whitened so t hat E[X]=0andE􏰂XXT􏰃=Id
"Unsupervised Learning
",Dimension Reduction,4,2,"In practice, this can be achieved by transforming the raw data to centred principal components and normalising them by their standard deviations"
"Unsupervised Learning
",Dimension Reduction,4,2,"The data x1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",xn ∈ Rd on which ICA is performed are thought of as realisations of the random vector X"
"Unsupervised Learning
",Dimension Reduction,4,2,"Note that,since E[X]=0=E[S]andE􏰂XXT􏰃=Id =E􏰂SST􏰃,the matrix A is necessarily orthogonal"
"Unsupervised Learning
",Dimension Reduction,4,2,"Hence, we look for an orthogonal matrix A such that has independent and non-Gaussian components"
"Unsupervised Learning
",Dimension Reduction,4,2,"To measure independence, entropy can be used"
"Unsupervised Learning
",Dimension Reduction,4,2,We recall that the differential entropy of a random variableY with probability density function (pdf) gY is
"Unsupervised Learning
",Dimension Reduction,4,2,"It is known that among all continuous distributions with fixed mean and variance, the corresponding normal distribution has the maximal entropy"
"Unsupervised Learning
",Dimension Reduction,4,2,"4 Using differential entropy, we can define the mutual information between the components Y1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",Yd of a random vector Y"
"Unsupervised Learning
",Dimension Reduction,4,2,It is possible to show using Jensen’s inequality that I (Y ) ≥ 0
"Unsupervised Learning
",Dimension Reduction,4,2,"Moreover, value zero is attained when the components of Y are mutually independent"
"Unsupervised Learning
",Dimension Reduction,4,2,"Thereby, we try to make Y1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",Yd not only as mutually independent as possible but also as non-Gaussian as possible"
"Unsupervised Learning
",Dimension Reduction,4,2,"Working with the differential entropies H(Y1),"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",H(Yd) in (6"
"Unsupervised Learning
",Dimension Reduction,4,2,"2) using sample data is awkward, however, since they require knowledge of the pdfs of Y1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",Yd, which in turn depend on the matrix A we are optimising over"
"Unsupervised Learning
",Dimension Reduction,4,2,"To make ICA feasible, it is standard to first replace each H (Yi ) with negentropy (a contraction of “negative entropy”)"
"Unsupervised Learning
",Dimension Reduction,4,2,"The negentropy J (Yi ) is another measure of how much Yi departs from Gaussianity  and since a normal distribution maximises entropy, we have J(Yi)≥0"
"Unsupervised Learning
",Dimension Reduction,4,2,"The negentropy J (Yi ) per se is not any more tractable than the original differential entropy H (Yi ), but it is amenable to approximation"
"Unsupervised Learning
",Dimension Reduction,4,2,"There are several different forms of function G that could be used, but Hyvärinen and Oja (2000) recommend G(x) := a1 logcosh(ax) with a ∈ [1,2]"
"Unsupervised Learning
",Dimension Reduction,4,2,"Finally, the expectation E[G(Yi )] in (6"
"Unsupervised Learning
",Dimension Reduction,4,2,3) is estimated using the corresponding sample mean and optimisation is then carried out using Newton’s method
"Unsupervised Learning
",Dimension Reduction,4,2,It is worthwhile to be aware of a few caveats of ICA which are not shared by PCA
"Unsupervised Learning
",Dimension Reduction,4,2,"Firstly, ICA is unable to recover the variances of the independent components"
"Unsupervised Learning
",Dimension Reduction,4,2,"Hence, it was of no loss of generality to assume at the outset that the variances are equal to unity"
"Unsupervised Learning
",Dimension Reduction,4,2,"Moreover, ICA is unable to recover the signs of the components, but this is also a limitation of PCA"
"Unsupervised Learning
",Dimension Reduction,4,2,"Secondly, the ordering of the independent components is arbitrary and conveys no information about their importance (unlike the ordering of principal components)"
"Unsupervised Learning
",Dimension Reduction,4,2,"Multidimensional scaling (MDS) in its simplest form is a dimensionality reduction method where we start with samples x1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",xn ∈ Rd in a high-dimensional space, which we seek to map to samples z1"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",zn ∈ Rq in a lower dimensional space, where q ≤ n, while preserving the geometry of the samples as much as possible"
"Unsupervised Learning
",Dimension Reduction,4,2,"The preservation of geometry can be measured through distances, or dissimilarities"
"Unsupervised Learning
",Dimension Reduction,4,2,"More broadly, MDS can also be applied when we only have access to the dissimilarities di,j , i, j = 1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",n, or to the similarities si,j , i, j = 1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",n, rather than the actual samples x1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",xn"
"Unsupervised Learning
",Dimension Reduction,4,2,"In this way MDS is fundamentally different from the dimensionality reduction methods we have studied previously, which all require the full coordinate data x 1 ,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,", x n"
"Unsupervised Learning
",Dimension Reduction,4,2,"Moreover, the distances di,j , i, j = 1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",n, need not be Euclidean distances — they could as well be geodesic distances on a manifold, say (which is a point we will touch upon in Section 7"
"Unsupervised Learning
",Dimension Reduction,4,2,"2, below)"
"Unsupervised Learning
",Dimension Reduction,4,2,Different formulations of MDS mainly differ by whether they use dissimilar- ities or similarities and how they quantify (7
"Unsupervised Learning
",Dimension Reduction,4,2,1) or (7
"Unsupervised Learning
",Dimension Reduction,4,2,2)
"Unsupervised Learning
",Dimension Reduction,4,2,"Classical scaling, also known as Torgerson–Gower scaling (Torgerson, 1958), is based on similarities and seeks to minimise the objective function, known as strain"
"Unsupervised Learning
",Dimension Reduction,4,2,"The strength of this approach is that the minimisation problem can be solved analytically, resembling ordinary PCA"
"Unsupervised Learning
",Dimension Reduction,4,2,"Let S:=􏰔si,j􏰕ni,j=1 ∈Rn×n, and let λ1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",λn ≥ 0 and u 1 ,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,", u n ∈ Rn be the eigenvalues and eigenvectors, respectively, of S"
"Unsupervised Learning
",Dimension Reduction,4,2,"Least squares scaling, or Kruskal–Shephard scaling (Kruskal, 1964), uses dissimilarities instead and is found by minimising the objective function, known as stress"
"Unsupervised Learning
",Dimension Reduction,4,2,"Despite their superficial similarity, classical scaling and least squares scaling are not equivalent"
"Unsupervised Learning
",Dimension Reduction,4,2,"Least squares scaling is not analytically solvable, but a gradient descent algorithm can be used"
"Unsupervised Learning
",Dimension Reduction,4,2,"Sammon mapping (Sammon, 1969) is a modification of least squares scaling where weighting by the reciprocals of the dissimilarities is implemented, i"
"Unsupervised Learning
",Dimension Reduction,4,2,e
"Unsupervised Learning
",Dimension Reduction,4,2,", Dimensionality reduction is most effective in situations where we analyse mul- tivariate data in Rd that are confined (or approximately confined) to a lower di- mensional subset of Rd"
"Unsupervised Learning
",Dimension Reduction,4,2,"If this subset is a linear space, then linear methods such as ordinary PCA apply"
"Unsupervised Learning
",Dimension Reduction,4,2,"However, more generally the subset could be a (non- linear) manifold (for example a sphere in R3)"
"Unsupervised Learning
",Dimension Reduction,4,2,"In kernel PCA, we would try to find (at least implicitly) a feature map that (approximately) linearises the manifold"
"Unsupervised Learning
",Dimension Reduction,4,2,But an alternative approach to non-linear dimensionality reduction is to try to emphasise the preservation of the local geometry of the data
"Unsupervised Learning
",Dimension Reduction,4,2,The motivation for such an approach comes from the very definition of a manifold
"Unsupervised Learning
",Dimension Reduction,4,2,"We recall that, informally, a q -manifold M in Rd , with q ≤ d , is a subset of Rd such that forany x ∈ M , the neighbourhood of x on the manifold resembles a neighbourhood in Rq"
"Unsupervised Learning
",Dimension Reduction,4,2,"Thus, we seek a mapping of the data in Rd to a lower dimensional space such that the local linear structure or the distances between neighbouring samples are preserved as much as possible"
"Unsupervised Learning
",Dimension Reduction,4,2,Isometric feature mapping (ISOMAP) (Tenenbaum et al
"Unsupervised Learning
",Dimension Reduction,4,2,", 2000) approximates the distance between each pair samples along the manifold by the shortest path between them on a graph constructed from nearest neighbour relationships"
"Unsupervised Learning
",Dimension Reduction,4,2,"Sub- sequently, MDS is applied to these approximate distances to form the mapping to a lower dimensional space"
"Unsupervised Learning
",Dimension Reduction,4,2,"Local linear embedding (Roweis and Saul, 2000) develops first an approximation of each sample as a linear combination of its neighbouring samples"
"Unsupervised Learning
",Dimension Reduction,4,2,"In other words, this estabishes an approximate linear relationship between the sample and its nearest neighbours"
"Unsupervised Learning
",Dimension Reduction,4,2,Points in a lower dimensional space are then sought so that they preserve these linear relationships as closely as possible
"Unsupervised Learning
",Dimension Reduction,4,2,"Local multidimensional scaling (local MDS) (Chen and Buja, 2009) is an extension of ordinary MDS, which we shall study in a bit more detail here"
"Unsupervised Learning
",Dimension Reduction,4,2,"Local MDS first determines a set N ⊂ {1,"
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,
"Unsupervised Learning
",Dimension Reduction,4,2,",n}2 neighbourhood relations such that (i,j)∈N if sample xi is among the K-nearest neighbours of sample xj or vice versa"
"Unsupervised Learning
",Dimension Reduction,4,2,"The first term encourages mapping that preserves distances between neighbouring samples, while in the second term D is chosen large so that non-neighbouring samples are mapped far apart by default"
"Unsupervised Learning
",Dimension Reduction,4,2,"However, simultaneously w is chosen small so that the second term gets deprioritised"
"Unsupervised Learning
",PDE,4,2,"Denoting by Xi,1,...,Xi,d the components of X i for any i = 1,...,d,"
"Unsupervised Learning
",PDE,4,2,"We assume that X1,...,Xn are d-dimensional random vectors that all follow a probability density function (pdf) gX ."
"Unsupervised Learning
",PDE,4,2,"Given a data set x1,...,xn ∈ Rd, which are seen as realisations of X1,...,Xn, our task is to try to estimate the underlying pdf gX ."
"Unsupervised Learning
",PDE,4,2,"In parametric density estimation, we postulate that there is a family G of pdfs g(·;θ) parameterised by θ ∈ Θ, where Θ is a subset of Rp for some p ∈ N, such that the true pdf gX is a member of this family, namely, for some (unique) θ0 ∈ Θ"
"Unsupervised Learning
",PDE,4,2,"Then our task is to estimate θ0, the difficulty of which
depends on the breadth and characteristics of the family G ."
"Unsupervised Learning
",PDE,4,2,"The family G could, for example, consist of all d -dimensional multivariate normal distributions, which can be parameterised by their mean μ ∈ Rd and covariance matrix Σ ∈ Rd ×d , i.e., θ = (μ, Σ)."
"Unsupervised Learning
",PDE,4,2,"Recalling that a covariance matrix is always positive semi-definite, where Sd≥0 is the space of symmetric positive semi-definite d × d matrices"
"Unsupervised Learning
",PDE,4,2,"Since
the covariance matrix Σ as a symmetric matrix has at most d (d −1) free parameters, 2
22 we can embed Θin the space"
"Unsupervised Learning
",PDE,4,2,"In non-parametric density estimation, we do not postulate a parametric family of pdfs, but in principle instead seek a pdf from the universe of all pdfs on Rd ."
"Unsupervised Learning
",PDE,4,2,"However, the non-parametric density estimators we use in practice, histograms and kernel density estimators, do not cover entire universe of all pdfs, but are instead able to approximate almost any pdf in the asymptotic limit where the number of samples n tends to infinity."
"Unsupervised Learning
",PDE,4,2,We will return to non-parametric density estimation shortly.
"Unsupervised Learning
",PDE,4,2,The strength of parametric density estimation vis-à-vis its non-parametric counterpart is (typically) better efficiency (in terms of lower error variance).
"Unsupervised Learning
",PDE,4,2,"But this holds provided G is correctly specified (the true pdf belongs to G ), which can be difficult to ensure in practice."
"Unsupervised Learning
",PDE,4,2,"As a rule, parametric estimation is better suited to situations where the number of samples is small, and efficiency is important, while non-parametric methods become more competitive as the number of samples increases."
"Unsupervised Learning
",PDE,4,2,"Maximum likelihood is the standard method for parametric density estimation. Returning to the setting of Section 8.1 and assuming that the random vectors X1,...,Xn are mutually independent, their joint pdf can be written as"
"Unsupervised Learning
",PDE,4,2,"Fixing the observed values x1,...,xn∈Rd and releasing the parameter value θ0∈
Θ, we obtain the likelihood function"
"Unsupervised Learning
",PDE,4,2,"The maximum likelihood estimate of θ, denoted by θ􏰅 is then defined as the maximiser of the likelihood function,"
"Unsupervised Learning
",PDE,4,2,"The likelihood function is defined as a product (8.4), which is often computationally cumbersome to deal with. Hence, it is convenient to take a logarithmic transform of it, giving rise to the log-likelihood function which is additive."
"Unsupervised Learning
",PDE,4,2,"Since x 􏰀→ log x is strictly increasing, L(θ) and l(θ) are equiva-
lent as far as maximisation is concerned."
"Unsupervised Learning
",PDE,4,2,"Thereby, the maximum likelihood estimate can equivalently be obtained by maximising the log-likelihood,"
"Unsupervised Learning
",PDE,4,2,"The maximum likelihood estimate θ􏰅=θ􏰅(x1,...,xn) is a non-random quantity that depends on the observations x1,...,xn."
"Unsupervised Learning
",PDE,4,2,"In some theoretical developments, it is useful to distinguish it from the corresponding estimator where x1,...,xn have been replaced by the underlying random vectors X1,...,Xn,i.e., which is a random variable."
"Unsupervised Learning
",PDE,4,2,"The log-likelihood function can be always adjusted by adding (or subtracting) a term that depends on the data x1,...,xn but not on the parameter vector θ, without changing the maximiser."
"Unsupervised Learning
",PDE,4,2,"In general, the maximum likelihood estimate need not be unique (there may be multiple maximisers) but in most practically relevant cases it is."
"Unsupervised Learning
",PDE,4,2,"For some distributions, the maximum likelihood estimate can be solved an- alytically, like in the following example. But more often than not, one needs to resort to numerical optimisation."
"Unsupervised Learning
",PDE,4,2,"The edge case where Σ is not invertible is troublesome and must be ruled out since the above pdf is then ill-defined. Hence, we assume that Σ is positive definite (not merely positive semi-definite)."
"Unsupervised Learning
",PDE,4,2,"Asymptotic theory has been derived to validate maximum likelihood estimation; see, e.g., van der Vaart (1998, Chapter 5)."
"Unsupervised Learning
",PDE,4,2,"Under rather general conditions, maximum likelihood estimators are consistent in the sense that the maximum likelihood estimator θ􏰅 converges to the true parameter value θ0 as the number of samples n tends to infinity."
"Unsupervised Learning
",PDE,4,2,"Similarly, the estimator can rather generally be shown to be asymptotically normal, which means that the rescaled estimation error converges towards a multivariate normal distribution as n → ∞."
"Unsupervised Learning
",PDE,4,2,Asymptotic normality is useful in inference (construction of confidence intervals and hypothesis tests).
"Unsupervised Learning
",PDE,4,2,"Remark8.13. We assumed at the outset that the random vectors X1,...,Xn generating the data x1,...,xn are mutually independent."
"Unsupervised Learning
",PDE,4,2,"Independence is crucial for the joint pdf (8.3) to hold, but it is not essential for the validity of the maximum likelihood estimator in general."
"Unsupervised Learning
",PDE,4,2,"It is possible to show that the maximum likelihood estimator remains consistent and asymptotically normal under quite general forms of dependent observations, even when the depen- dence is neglected in the derivation of the estimator."
"Unsupervised Learning
",PDE,4,2,"For example, in the case of the multivariate distribution the sample mean and covariance matrix are consistent estimators of the population mean and covariance matrix, respectively, under dependent observations satisfying a weak property known as ergodicity."
"Unsupervised Learning
",Estimation of Mixtures,4,2,A common difficulty with applying parametric density estimation to large data sets is that standard parametric families of pdfs struggle with accommodating the complexities of the data.
"Unsupervised Learning
",Estimation of Mixtures,4,2,Data sets where samples appear in multiple clusters pose a particular problem as they would necessitate a multimodal distribution — a density with multiple peaks.
"Unsupervised Learning
",Estimation of Mixtures,4,2,"Fortunately, multimodal distributions are easy to build from standard families of distributions using mixtures."
"Unsupervised Learning
",Estimation of Mixtures,4,2,"Let q ≥ 2 and let g1( · ;θ1),...,gq( · ;θq) be pdfs on Rd parameterised by θ1,...,θq, respectively."
"Unsupervised Learning
",Estimation of Mixtures,4,2,"As a rule, the mixture model is more convenient to analyse using a generative
representation with a latent variable."
"Unsupervised Learning
",Estimation of Mixtures,4,2,"To this end, we introduce a d-dimensional random vector X and a categorical random variable Z with values in {1,...,q} such that"
"Unsupervised Learning
",Estimation of Mixtures,4,2,"Then X follows the pdf h(·), given by (9.1), and Z serves as an indicator of which component of the mixture the observed value stems from."
"Unsupervised Learning
",Estimation of Mixtures,4,2,"Observing X only, Z remains latent, however."
"Unsupervised Learning
",Estimation of Mixtures,4,2,This is called the Gaussian mixture model.
"Unsupervised Learning
",Estimation of Mixtures,4,2,"To estimate the mixture model, we could in principle proceed with maximum likelihood using the pdf (9.1)."
"Unsupervised Learning
",Estimation of Mixtures,4,2,"Unfortunately, the pdf as a sum does not play nicely with multiplication and logarithms."
"Unsupervised Learning
",Estimation of Mixtures,4,2,"Hence, this approach is in most cases a non- starter."
"Unsupervised Learning
",Estimation of Mixtures,4,2,"Fortunately, mixture models can be estimated more efficiently using the expectation–maximisation (EM) algorithm, popularised by Dempster et al. (1977)."
"Unsupervised Learning
",Estimation of Mixtures,4,2,"The EM algorithm is based on the representation using the latent variable Z ; equations (9.2) and (9.3). For conve- nience, we collect all the parameters θ1,...,θq and π1,...,πq into a single vector θ."
"Unsupervised Learning
",Estimation of Mixtures,4,2,"Suppose that the data x1,...,xn are generated by mutually independent randomvariablesX1,...,Xn ∼h(·)andthatZ1,...,Zn arethecorrespondingcat- egorical latent variables with realised values z1,...,zn."
"Unsupervised Learning
",Estimation of Mixtures,4,2,The EM algorithm proceeds now iteratively.
"Unsupervised Learning
",Estimation of Mixtures,4,2,"At each iteration, we keep the current parameter values stored in θ(t)."
"Unsupervised Learning
",Estimation of Mixtures,4,2,"The EM algorithm is not guaranteed to converge to a global maximum, but merely to a local one."
"Unsupervised Learning
",Estimation of Mixtures,4,2,"Hence, if there are doubts about the optimality of the end result, experimentation with different initial values is warranted."
"Unsupervised Learning
",Estimation of Mixtures,4,2,This is analogous to maximum likelihood estimation of the multinomial distribution and the solution (which can be obtained using the Lagrange multiplier method) is
"Unsupervised Learning
",Estimation of Mixtures,4,2,"The solution can be obtained by modifying the derivation of the maximum likelihood estimate in the (standard) multivariate normal setting (Example 8.8 in Lecture Note 8), yielding"
"Unsupervised Learning
",Histograms,4,2,Histograms are used ubiquitously to visualise distributions of samples. But they are also the simplest form of a non-parametric density estimator.
"Unsupervised Learning
",Histograms,4,2,"While histograms could in principle be constructed for any dimensionality of data, their visualisation becomes difficult in higher dimensions, save for two."
"Unsupervised Learning
",Histograms,4,2,"Hence, we will develop here a histogram for one-dimensional data x1,...,xn ∈ R only."
"Unsupervised Learning
",Histograms,4,2,"To construct the histogram, the real line is divided into disjoint intervals, bins."
"Unsupervised Learning
",Histograms,4,2,"The bins need not be of equal size but they are commonly chosen to be so, an assumption we will also make here."
"Unsupervised Learning
",Histograms,4,2,"A quantity closed related with the bin width h is the bin count, i.e., the smallest number of bins that cover the entire data set x1,...,xn."
"Unsupervised Learning
",Histograms,4,2,"To understand why the histogram is a density estimator, we study its properties when the data x1,...,xn have been generated by mutually independent and iden- tically distributed random variables X1,...,Xn with pdf f , i.e.,"
"Unsupervised Learning
",Histograms,4,2,This study also paves the way for the practically relevant question how to choose the bin width (or bin count).
"Unsupervised Learning
",Histograms,4,2,We start by working out the expectation of the histogram:
"Unsupervised Learning
",Histograms,4,2,"If the pdf f is a continuous function and if we let the bin width shrink to zero,
h → 0, we get1"
"Unsupervised Learning
",Histograms,4,2,"While the numerator tends to f (x) as h → 0, the denominator tends to zero, exploding the variance."
"Unsupervised Learning
",Histograms,4,2,This shows that there is bias–variance tradeoff — smaller bin size results in lower bias but also higher variance.
"Unsupervised Learning
",Histograms,4,2,"Theoretically, if the number of samples, n, tends to infinity, then we can let the bin side h go to zero with asymptotically vanishing variance provided"
"Unsupervised Learning
",Histograms,4,2,"In practice, the bin width
(or bin count) should thus be tied to the number of samples."
"Unsupervised Learning
",Histograms,4,2,"Since the variance of the histogram depends on the pdf f itself, there is no uni- versal, optimal way of selecting the bin width h (or bin count K ) at the outset."
"Unsupervised Learning
",Histograms,4,2,"However, various quasi-optimal rules have been proposed under different assumptions, of which the following three are among the best known:"
"Unsupervised Learning
",Histograms,4,2,"It is optimised, in terms of mean square error, for normally distributed data."
"Unsupervised Learning
",Histograms,4,2,The interquartile range makes the formula more robust to the extreme observations compared to Scott’s formula.
"Unsupervised Learning
",Histograms,4,2,For visualisation purposes the bin width (or bin count) choice is ultimately subjective.
"Unsupervised Learning
",Histograms,4,2,Does one prefer a smoother or noisier representation of the data?
"Unsupervised Learning
",Histograms,4,2,There are good reasons to prefer a slightly noisy histogram.
"Unsupervised Learning
",Histograms,4,2,"If a smoother representation is sought, it may be better to switch to a kernel density estimator (next topic) anyway."